{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210a1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38e7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef59d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.youtube.com/watch?v=XQgXKtPSzUI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd197d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "option =  webdriver.ChromeOptions() \n",
    "option.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()), options=option)\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "\n",
    "# https://stackoverflow.com/questions/51690101/why-execute-scriptreturn-docume\n",
    "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "# https://www.codegrepper.com/code-examples/python/how+to+scroll+to+the+end+of+the+page+in+selenium+python\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, \" + str(last_height) + \");\")\n",
    "    # Wait to load page\n",
    "    time.sleep(1)\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    \n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "    \n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "driver.quit()\n",
    "\n",
    "# https://stackoverflow.com/questions/38028384/beautifulsoup-difference-between-find-and-select\n",
    "title = soup.select_one('#container > h1').text\n",
    "\n",
    "comments = soup.select(\"#content > #content-text\")\n",
    "comment_list = [comment.text for comment in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395033b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intro to Web Scraping with Python and Beautiful Soup'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd2de59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very easy to understand and to follow. Good explanation, with good content. Thank you for sharing your knowledge.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_list[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694e0101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone, find the updated version of this tutorial here: https://www.youtube.com/watch?v=rlR0f4zZKvc&list=PL8eNk_zTBST-SaABhXwBFbKvvA0tlRSRV&index=3',\n",
       " 'Table of Contents:\\n0:00   -   Introduction\\n1:28   -   Setting up Anaconda\\n3:00   -   Installing Beautiful Soup\\n3:43   -   Setting up urllib\\n6:07   -   Retrieving the Web Page\\n10:47 -   Evaluating Web Page\\n11:27 -   Converting Listings into Line Items \\n16:13 -   Using jsbeautiful \\n16:31 -   Reading Raw HTML for Items to Scrape\\n18:34 -   Building the Scraper\\n22:11 -   Using the \"findAll\" Function\\n27:26 -   Testing the Scraper\\n29:07 -   Creating the .csv File\\n32:18 -   End Result',\n",
       " \"This was by far the best introduction to web scraping I've found online. Clear, concise, and easy to digest. Thank YOU!\",\n",
       " \"It's weird to think about it like that, but this video started my whole Python learning back in 2017 and I am SO SO SO much thankful for it.\",\n",
       " 'MINOR SUGGESTION\\nAs of 10/03/2019, If you are following along this tutorial. \"container.div\" won\\'t give you the div with the \"item-info\" class. Instead it will give you the div with the \"item-badges\" class. This is because the latter occurs before the former. When you access any tag with the dot(.) operator, it will just return the first instance of that tag. I had a problem following this along until i figured this out. To solve this just use the \"find()\" method to find exactly the div which contains the information that you want. For e.g.   divWithInfo = containers[0].find(\"div\",\"item-info\")',\n",
       " 'Thank you for a great video. For someone a newbie, this was a bit confusing, since you need to add paths for both python and pip separately in order to run them from the command line. :)',\n",
       " 'If you had some prior experiences with web crawling, this video can makes your crawling skills into a whole new level. Allows you to crawl website containing complicated info about multiple items into a very organized dataset. The various tools introduced in the video are also fantastically helpful as well. A BIG THANK YOU',\n",
       " \"This was really good content, definitely the best intro to web scraping I've seen. You don't go through it as though you're reading from the documentation, there's more of a flow.\",\n",
       " 'Really great video! Only just starting with python and this was very helpful! Thanks ',\n",
       " 'Thanks. I have a basic understanding of python and html and I found this tutorial very easy to follow. You do a great job of clearly explaining things in the code which is what I need at my current skill level. Much appreciated.',\n",
       " \"Absolute champion, quite possibly the best code tutorial I've ever watched. Oh the possibilities! Thank you :)\",\n",
       " 'As of Nov 2020, I went through the whole thing without any issue!  I used a different product name, but everything worked so well!   Everything worked so perfectly! I learned so much from this video!  this is awesome!!!!  Thank you!!!!',\n",
       " \"That was great. Great work. I'm a beginner in Python, so it was a little bit advanced for me. If I could give one tip, it would be that as you get further and further into the details of the script, it's rather easy to lose the plot, that is, what we're doing, what we've done, and what we still have to do, so it wouldn't be any harm at certain stages to recap for the audience, saying where we are and what we did and still have to do. Just an audience thing to keep people with you. Apart from that, excellent tutorial.\",\n",
       " '32:30, I started cheesing at how awesome the end result of this whole project was.  Definitely inspiring - thank you for the excellent guide!',\n",
       " 'Truly enjoyed your simple step by step explanation on why each command or function is needed, and what it does.  Your Python knowledge and skills are evident, as you  are able to provide immediate solutions to errors and or challenges to the problem you are attempting to solve.  Followed along with the tools and enjoyed the session. Thank you.',\n",
       " 'Great content! I\\'ve watched dozens (hundreds?) of coding vids on youtube and I normally up the speed and skip around because they tend to go really slowly. You did a really good job of moving quickly while also covering most of the little details that beginners would get stuck wondering, like pressing \"up\" in the console to repeat your last command.\\n\\nMy only constructive feedback is that the font was a little bit too big. While I would much rather it be too big than too small, there\\'s a happy medium somewhere in the middle a few sized down from what you were displaying, mostly just when displaying the html.\\n\\nAgain, great vid and I\\'ll definitely check out some more of your stuff!',\n",
       " \"Thank you so much for the videos Data Science Dojo! I'm a data analyst working in Hungary, and I received a small scraping task to do in the last few days. I had a previous notion of how requests and BeautifulSoup worked, however, your video brought me from zero to hero to finish the task. Keep the good work!\",\n",
       " 'Two years into a web program and a year working in the field and never bothered to learn how to do this. Great video, I followed along 5 years later in 2022 with Python 3.7.8 and it still works.',\n",
       " 'Fantastic tutorial! gave me 95% of what I needed for my first screen scraping project.',\n",
       " \"THANK YOU SO MUCH! \\n\\nHad to put that in all caps. The best comprehensive web scraping  tutorial I've come across. Thanks again.\",\n",
       " 'Very easy to understand and to follow. Good explanation, with good content. Thank you for sharing your knowledge.',\n",
       " \"Fantastic video dude, much more helpful than others I've seen on Youtube\",\n",
       " \"Thanks for this... it's amazing. You're a natural teacher. Is there any chance you could make a video about how to scrape a table and output the data into a spreadsheet? I'm having a hard time with this and there are really no videos out there as good as yours.\",\n",
       " 'This tutorial is everything I searched for. Thank you for covering it that broadly and including several ways to get differenty kinds of information, both the constant and the varying ones. That\\'s really something that sets this video apart from all the other tutorials.\\n\\nNow that I\\'ve come quite far with my code, I\\'ve stumbled upon one exception and maybe you can help me here:\\n\\nThe code snippets that I try to collect from my file with \"find_all\" (in your case the containers) sometimes contain line breaks. This leads to the function making one entry per line and thus splitting a text where it should stay together and creating incomplete entries.\\nDo you have an idea how I can prevent this from happening? Since the function isn\\'t a string, I can\\'t just append \".replace\" or something so what would be the best way to solve this?',\n",
       " \"I can't thank you enough for this tutorial. Im currently working in course project of creating a database extracting data from the web, and this tutorial was a big insight for me.\",\n",
       " 'THIS IS AMAZING!!! Everything was very well-explained and instructed, I managed to get my first webscrape off an E-commerce site! Thanks so much, you have a loyal subscriber in me!\\n\\nPerhaps you could cover using time sleeps to avoid getting blacklisted by the websites we are scraping? And also how to scrape multiple pages in one go?',\n",
       " 'Great video!!!! Really useful. It would be Awesome if you could make an updated one for 2019. A lot of people would appreciate that(trust me)',\n",
       " \"This is so helpful! The video has been really well put together and was answering my questions as they came to me. I'll probably be watching this a few more times as I practice! Thank you!!!\",\n",
       " 'This is the best web scraping tutorial that I’ve found. I’ve been frustrated for hours trying to use other resources. Thank you for making this, your explanations are thorough and great!',\n",
       " 'Great video! You made everything crystal clear and understandable',\n",
       " \"This is REALLY good content.  I know I'm not speaking for myself, but I would love to see a video of you doing this same thing but having it run once an hour and have it send an email if there is a change.\",\n",
       " 'Coming from an R user, this is a very well done introductory tutorial into web scraping in Python. I like the real world example with Newegg and troubleshooting along the way.',\n",
       " \"Awestruck! It's amazingly simple to follow along! Thank you, sir, for adding to the community of self-learners!\",\n",
       " \"Love this tutorial\\nYou explained it so well that it's really easy to follow\\nThanks\",\n",
       " \"Awesome tutorial!!! I don't even know any python and I was able to follow just fine. Really excited to dive into my first web scraping project :) Thanks!\",\n",
       " 'This was genuinely an amazing video. Thanks very much. Very, very useful.\\n\\nAny chance you could do another, this time on web scraping with cookies?\\n(I.e. with login details, age checks, locations, etc.)',\n",
       " 'One of the best teacher I have come across Youtube. Web Scraping explained so well that even a layman can follow and understand the basic concepts. I wish, in life I had a teacher/mentor/friend like the one teaching in this video.',\n",
       " 'This is very well explained and I enjoyed every second of it ! please do more ^^',\n",
       " 'Great and clear tutorial for beginners. Cheers!',\n",
       " 'I am just starting web scrapping and I can honestly say that this video clearly explained everything. I watched this at 1.5 speed and it made sense. I would love more videos like this. I loved how you made it generic so it can apply to more than one website!',\n",
       " 'Hey there! this guide really helped me to create a tailored scraper for a pilot project. Even though I am at the very beginning stage of learning Python, I could manage to create the entire script, and even learned while the process. Amazing, really appreciate this!',\n",
       " \"Thanks for the video, really enjoyed it! I have one remark though.\\n\\nDon't you think that you could have used find() instead of find_all() inside the loop. \\nI think that find_all() might be redundant in this case, because you are extracting only one tag.\\n\\nI might be wrong though, please correct me if I am!\",\n",
       " 'Your presentation and explanation are awesome!  You have opened my eyes to the uses of Python and Beautiful Soup.',\n",
       " \"A BIG BIG THANK YOU: the most understable tutorial I've ever seen on how to scrape a web page (and I have visionned like 100 of them)\",\n",
       " 'I cant believe I actually sat through 33 minutes learning web scrapping, something completely new to me. I was looking for a shortcut but your tutorial was just perfect! :D Thanks for this.!',\n",
       " 'DUDE! High Quality Content!! You are very good at walking through the logical steps for breaking down a page! Other tutorials are great but are always geared toward the specific task at hand. With this it felt like I also learned how to tackle a page! \\n\\nThis helped a bunch!',\n",
       " 'Amazing stuff man. Very well explained. Do a tutorial on parsing xml and text files in python? That should be really helpful too, especially where we have to analyze log files for instance',\n",
       " 'As someone self learning Python (my first programming language) with a web scraping script in mind, this was great!',\n",
       " \"Very high-quality tutorial.\\nHow to set up everything before running any code is very nice to include,  and timestamping it so people who already know it can quickly skip is just much appreciated.\\nKeeping the tutorial example script and diverse is very welcome.\\nWriting it from scratch just makes sooo useful for remembering what was where.\\n\\nI wish other people made tutorials like this... Timestamping is so useful when you just want to look-up that one thing and don't really remember when it appeared.\",\n",
       " 'This material is just amazing. Thank you! Have you considered making an intro to Web Scraping using R?',\n",
       " 'As a newbie I can tell this helped a lot, thanks mate. Excellent job.',\n",
       " 'Super tutorial, you go straight to the point ! ',\n",
       " \"Great stuff.  I like python but I'm wondering about doing scrapping inside a PHP app like laravel,  are there any good packages?  I've heard of goutte but it would be cool to see a PHP based scraper that scrapes after X amount of time.\",\n",
       " 'Very helpful. Thank you. I followed step by step, and used another webpage for the scrapping but somehow the structure of that webpage did not allow me to find the correct html tags to use for extraction. Will continue to try.',\n",
       " 'Amazing video, very clear and easy to follow. Best intro ive come across',\n",
       " 'I was able to make a program for my client i never thought was possible. I got paid real money for this.\\nBlessings so much learned, this is like magic',\n",
       " 'Amazing content, very very helpful. Thank a lot for taking the time in doing this.',\n",
       " 'This was a very cool (and practical) video.  Thanks for publishing!',\n",
       " 'Awesome tutorial man! exactly what I was looking for',\n",
       " 'Fantastic! Thank you very much! Very clear and detailed tutorial!',\n",
       " 'You are the most concise teacher of python I have come across \\nThanks \\nI will definitely give your other videos a view',\n",
       " \"Thank you very much for this video!\\n\\nI hope you do a second one on this subject. I'd like to know how to scrape several pages as you mentioned in the end of video. This was just what I was hoping for. Thanks!\",\n",
       " 'Hello, thanks or sharing this great tutorial. I tried scraping following this tutorial and it was a great success. However I am stuck in another script where i need to grab similar posts and every post contained inside in each single list or <li>. The problem is the class name inside this <li> contains a post number which varies from <li> to <li>. The list looks like this::  <li class=\" post post-81993 type-post status-publish format-standard has-post-thumbnail hentry category-crime\"> You can see the number there and each list that contains the post i need to grab contains a different number. So apparently if i write that class to grab that post it will only grab one single post, not all of them. Any kind of help would be greatly appreciated. Thanks!',\n",
       " 'Loved this video, clear and straight to the point, was able to make my first web scraping program',\n",
       " 'I thought web scraping was hard until I found your video. Huge thanks man, you saved so much time for me!',\n",
       " 'Thank you very much for this, very informative, would be very interested in learning combining this with the loop to go through the pages, or maybe aggregating data from more than one site.',\n",
       " 'For people in some countries the standard delimiter in CSV-files is a semicolon, so the entries will have to be separated by a semicolon if you dont want to have to change your system settings. just a FYI some of you might find useful',\n",
       " 'wow this was great! I am completely new to this and still could follow perfectly fine and loved the explanations of everything. Would love to know how to run this script every day automatically and send results to phone or create alerts for changes and send those to a phone. Again, awesome job!',\n",
       " 'Insanely easy to understand, comfortable to listen. Thank you a lot! :)',\n",
       " 'Please make more of this! This is the first tutorial I saw about web scraping and I understand totally. Thanks.',\n",
       " 'This is great! Would love to see how to handle edge cases. Also, how would you scrape the data from each product page, i.e. follow the link and store the description, tech specs,',\n",
       " 'Great job. I liked how you explain everything really well.',\n",
       " 'Thanks a lot! This video was extremely helpful and straightforward.',\n",
       " 'wow even almost 3 years later this video helped me so much and helped me to make a program that picks a random steam game, this was so hard, but i figured it out, big props to you and this video <3',\n",
       " 'I really liked the tone, rythm and clarity of this tutorial! I‘m not a total beginner with python anymore and so was able to listen and (mostly?) understand while preparing lunch for my kids. (I‘ll rewatch to try and do it later)',\n",
       " 'Awesome video man... Make these kinda practical videos with live coding.... Amazing stuff.. loved it!',\n",
       " 'You made it so easy to understand everything. Thank you so much, We need teachers like you really. Appreciate your quality of work. Keep Going and teaching and we will keep learning :D',\n",
       " 'Very concise and useful video thank you. \\n\\nFor my specific application, when I click on my product it takes my to a page that contains additional information that I would also like to scrap. Is there any way of incorperating this into the code somehow?\\n\\nThanks.',\n",
       " 'Unbelievably helpful! Thank you so much!',\n",
       " 'The man, the myth, the legend.\\n\\nYou have no idea how much stress and lost time you have prevented. THANK YOU!',\n",
       " 'Easy to follow and understand. Very nice tutorial. Thank you very much!',\n",
       " 'I learned SOOOO MUCH from this video, you are the BEST!!!!!!! Your explanation was super simple and just spot on, thank you SO MUCH!!!!!!!!',\n",
       " 'I love this type of lesson. Building useful software with real world applications.',\n",
       " 'Thanks a lot for this tutorial... Having a bit of difficulty with going through pages but I will get it eventually ',\n",
       " 'You could also use Visual Studio and add the Python module from the Visual Studio Installer.  This gives you the benefit of intellisense and a ton of other features of Visual Studio live having an interactive python window below your editor window.  It also makes it easy to search for packages and to install or remove them.',\n",
       " 'I saw this video and then successfully wrote the entire code without looking at the video. Not even once. This is because i understood every line of it. Thank you man. Your explanation is very beginner friendly.',\n",
       " 'Great Video.Thank you :D\\nHowever,i have one doubt.\\nAre comma seperated values automatically stored in different columns in a csv file?\\nPlease help.',\n",
       " 'Thank you very much! The video was very clear to understand, it helped me to progress in learning python.',\n",
       " 'It was really helpful to me. I was confused by some other videos but this one was great. Thank you',\n",
       " 'Great video! Thank you for sparing the time and explaining in such a straight forward delivery.',\n",
       " 'This is a really good lesson into web scraping. Thank you for demonstrating it.',\n",
       " 'Nice! I was wondering if you could do a page monitor where it tells you exactly where the website has changed?',\n",
       " 'You explained this amazingly. Hopefully can use python to its fullest',\n",
       " \"Never actually used python just before just learned basic syntax. Had a few obstacles, but luckily we have stack overflow. I can't believe that I am saying this but the tutorial was better than the one by Brad Traversey.\\n\\n\\nKeep it up.\\n\\n\\nCan you make a tutorial about django rest api?\",\n",
       " 'This is awesome. Thanks for the clearcut explanation and demonstration',\n",
       " 'This was well done. Thanks for the demonstration along with the explanation!',\n",
       " \"Great tutorial! I'm having trouble exporting to the csv though! When I print, my loop is working correctly and scraping all of the data that I want. But, when I export, I only get the data of one product... none of the other 50. When I enter my web scrape file name into the command prompt and I execute the code, I see all of the product data that I'm attempting to pull flash on my screen, yet only the last product appears in my csv. Any Ideas what's going on?\",\n",
       " \"Hello, awesome tutorial! But when I make the .csv file the data appears in one column separated by commas, not in individual columns. I've checked all the code and is exactly the same as the video code.\",\n",
       " 'You, sir, have beautifully explained the workflow of web scrapping. Thank you so much.',\n",
       " 'You are a blessing seriously! The first tutorial that actually made sense from start to finish. I was able to understand so much from this! Please Please Please Please upload more videos on Python Web Scraping with BeautifulSoup. \\n\\n\\n\\nThank you again for this blessing!',\n",
       " \"Everyone learns in a different way, and absorbs information through different methods!  This informal, laid back 'talk & walkthrough' (almost like sitting together with a mate) fits my style sooo much!! for me probably the best python lesson ever!! Will be looking for many more - thanks :P legendary !!\",\n",
       " \"Mate, this is just perfect! I learned so much by doing this with you. Now I'm ready to tackle other websites!!! You're a legend!\",\n",
       " \"Very good video!  great explanation of all the steps involved, I'll agree that this is best scraping video i've found .  Thanks!\",\n",
       " 'Great video. ..very easy to follow. hope you do more of that kind. Thanks.',\n",
       " \"Awesome video, thank you so much! I've just got one question. What is the difference between importing urllib.request and importing requests. I've noticed that if you import the second one, you can use thing like .get(url) , which does the same thing as ureq(url), right? If so, is one better to use than the other?\",\n",
       " 'Very useful and to the point, thank you!',\n",
       " 'This was perfect for me--exactly what I needed. Thanks so much!',\n",
       " 'This is one of the only clear|fun python tutorials out there. Congrats',\n",
       " \"Wow. Can't believe I just learned webscraping with beautiful soup in 30 mins. Great video. Fast pace and talk but that's what pause and rewind is for.\",\n",
       " 'Hi Dojo, Really nice video. I have one doubt. The recent eCommerce sites done have class items constant, they have alpha numeric values like   class=\"_3Hjcsab\" how do you scrape when the site keeps on changing?',\n",
       " \"This is the best tutoring for webscraping... In just one vedio I learned so much and understood very nicely. It was my first time and I didn't know even a inch but I just stuck to this one ,not even for once opened any other video.\\nIt's damn amazing. Thank you for making it and making it so well.\",\n",
       " 'Awesome video, it cleared my concepts of Bs4 beautifully...Thanks a lot for making easier for new learners like me.',\n",
       " \"Thank you for this!  It's cool and important (to me) to see what one would actually do with python.\",\n",
       " \"Amazing tutorial, however, I'm having one issue, when I save the csv file it looks good in excel but when I try to create a graph using matplotlib and pandas, it will give me a blank graph. I'm assuming this is data type issue as when I manually change the data to numeric values in excel it does work. Would you suggest any solution for this?\",\n",
       " 'Very clear.  One thing, you might consider a modern all in one dev tool and environment like PyCharm, or at least VisStudio. It will save you a lot of time vs Sublime, site tools, CMI, etc.',\n",
       " \"This was fast, precise and beautiful! By saying beautiful I didn't mean to state the obvious :) Thanks\",\n",
       " 'This is awesome. Thanks, man. I really enjoy your style of teaching.',\n",
       " 'This is very well explained. I like this as a beginners guide!',\n",
       " 'that was awesome man - so much appreciation for things like this! you could throw in adding the csv into a database - and then throw in a query on the best card!',\n",
       " 'Awesome video: right pace, great content, very useful tricks...!',\n",
       " 'It is so cool but as a newbie scrapper it took me a while to get used to some new terms and difficulties. Thanks a lot, Bless you!',\n",
       " 'Maybe it\\'s better to use find() instead of findAll() to get product\\'s name? So code will be less complex, like this : \\ntitle = container.find(\"a\",{\"class\" : \"item-title\"}).text',\n",
       " \"Great video, I think I'm going to like web-scraping.\\nYou seem to have covered all the bases, except how would you include the product image?\",\n",
       " 'Excellent video man! Just keep the terminal fonts smaller. That much large fonts distorts the format and hard to keep track.',\n",
       " 'Really great video. did manage to scrape a site well from how you explained it. good approach you took. one question i have about scraping the PRICE. in my situation they don\\'t seem to have consistent html class for price sometimes its \"price\" then in some products it might show \"rrp-price\" then \" now-price\". how can i do this without having to load it into 3 different variables. so i can pass the actual sale price no matter if its in a now-price class or not. do i need to use IF STATEMENTS here? if so i can work out how',\n",
       " 'This is one of the most useful Web Scrapping videos I have ever come across. I could learn it from scratch. Thanks.',\n",
       " 'This is the first tutorial on this that actually makes sense. THANK YOU. You earned a subscriber.',\n",
       " 'UPDATE/SUGGESTION \\nThe findALL function has been renamed to the find_all function in Bs4 version 4.9.3',\n",
       " \"The best tutorial on web scrapping, I've ever seen! Great work!\",\n",
       " 'Hi...thanks for the excellent tutorial. Really appreciate the way you have taken time to break down and explain each line of code.\\nI am trying to scrape Linkedin, in order to sort my connections by the number of likes/comments their posts garner. This is to ensure that I am latching on to the right posts.\\nIs this something possible to do with Python - if yes, could you help me with contouring the code or direct me to a source that can?\\nThanks',\n",
       " 'You are a great teacher I believe I was not much not into data Science  but after watching your video it made it simple and easy. Thank you I took 100% from it. :) requesting more videos from you..',\n",
       " 'Awesome!  this was pretty valid in 2020,    \\nI had to change findALL()  to find_all(\"div\", class_=\"item-container\")',\n",
       " 'Finally a good vid explaining how to data mining/scraping. I am still learning but very excited about it. Once you master this you can mine anything. Thanks for the vid mate.',\n",
       " 'enjoyed, data science ! Need more like this one',\n",
       " 'Absolutely brilliant! Thanks so much!',\n",
       " 'This was great, very easy to follow and fun to learn. THank you!',\n",
       " 'That was great and based on that fact, Ive sub\\'d and \"notified\". Thanks\\n\\nIm just starting out in data science (very later starter but really fascinates me) and all this is good knowledge. Id like to see the same concept but with a API call to a url and how to break down the return items and display them. \\n\\nWill go through your other videos. \\n\\nThanks again',\n",
       " \"dude you are amazing at teaching and at coding, I've been wanting to do this for so long... now I can, thank you really much\",\n",
       " 'I dont know much about coding but the way you explained this made perfect sense. I hope to learn a lot from your channel.',\n",
       " \"This has been so useful. Thanks so much. What I need to know now, is how I can get the scraper to continue working when there's a 'Load More' button, which doesn't take you to another page. If anyone knows anything about this please let me know.\",\n",
       " 'Very good and easy to understand. Thanks :)',\n",
       " \"Best tutorial I've seen. Thank you for saving me so much time!\",\n",
       " 'Thanks heaps, your explanation was perfect and the tutorial was very informative! :)',\n",
       " 'I have watched all the web scraping videos on YouTube but this one is the top, I learned a lot. Thank you.',\n",
       " 'Fan-freaking-tastic tutorial. Finally got this working the way I want it to. Thank you!',\n",
       " 'Hi, thanks for the video! How do you get to the second div tag in \"container\"?',\n",
       " \"Awesome tutorial man! I'm new to web scraping and I must say boy this is worth the watch!\",\n",
       " 'excellent tutor and very clear description...please do more',\n",
       " 'Thank you for this video. Very interesting! I have 1 question though. When I open my created CSV file. All the data is shown in 1 column. While it is seperated in 3 column in yours. I have used the exact same code as you have. How do I divide the headers with the according data in different columns? I use Python 3.6.5. Thank you! I am looking forward to your answer.',\n",
       " 'Great video! I\\'m a beginner and I really understood it all!\\nJust subscribed to your channel.\\n\\nI just have a little doubt.\\nAfter running the code, this message appears:\\nTraceback (most recent call last):\\n  File \"C:\\\\Users\\\\amato\\\\Downloads\\\\webcrawler.py\", line 19, in <module>\\n    brand = container.div.div.a.img[\"title\"]\\nTypeError: \\'NoneType\\' object is not subscriptable\\n\\nI wonder if someone may help me!',\n",
       " 'The explanation was super. Also thank you you for showing all those handy tools. Keep it up!',\n",
       " 'Hello, at 20:14 , the <div> tag (in my case) jumps to <div> tag inside <a> tag. How to choose which tag we want to grab if there is more than 1 tag with same name',\n",
       " \"Thank you so much, you're an excellent teacher!\",\n",
       " \"Thank you so much for this! ... I actually managed to create a scrapper to scrape all of the containers and output them to a csv based upon your tutorial... I would however like to know if there is an easy way to continue onward to the next page. I currently am changing the number in the reques url but i know there's a way that it can be automated ... is this something you can provide the code for? thank you once again...\",\n",
       " 'Awesome and to the point. Thank you so much',\n",
       " \"This is the best web scraping tutorial I've ever seen! Thank you so much!\",\n",
       " 'What just I saw! \\nThis is really .. really helpful. \\nYou deserve a big THANKS! \\nYou present everything not just fine but out of the box!\\nTHANKS AGAIN...!!!',\n",
       " 'Awesome tutorial, Please add how to scrap multiple pages :)',\n",
       " \"Thanks for the great video, I was able to scrape data with nearly no experience in coding. I only have one problem: I'm not able to scrape images from a website, is there anything special I have to look for for scraping images?\",\n",
       " \"dude, you are literally saving lives with this type of videos... I can't wait to digest all this precious info.....  You save people so much time with this!! you are magic!! <3 Thank you!!!!!!!!!!\",\n",
       " 'It is amazing and helpful, it exactly what I looking for. but I have a question: Is that possible to requests multiple webpages at once and read those links for that webpage from a txt file? Thanks in Advance ',\n",
       " 'This is incredible! Thank you so much.\\nI tweaked your code a bit because I guess newegg has changed their website from the time you created this video.',\n",
       " 'Looks like they added another div at the very beginning of each item-container. The brand name can now be extracted with a little more effort-\\nbrand_container = x.findAll(\"div\",{\"class\":\"item-info\"})\\nprint(brand_container[0].div.a.img[\"title\"])',\n",
       " 'Man, great great video! Pretty simple, fast and clarifying one.  Thx a lot.',\n",
       " 'Many thanks for the clear explanation !!\\r\\n \\nI only have one question. How can you find out the prices that are only visible in the shopping cart? Can you make a video about this? :D Thanks mate',\n",
       " \"Hi, this is a great video tutorial. Can you explain if i want to scrape inside those products? And if there's a pagination, how can i work with that? Thank you\",\n",
       " 'thank you so much for this!  So helpful and useful',\n",
       " 'Amazing tutorial!  Definitely worth watching!',\n",
       " 'Wow great video.Can you make a video on srapping data from multiple pages',\n",
       " 'This is a very well done content . Thank you !',\n",
       " 'thank you my brother. Very simple tutorial! As others mentioned.. use find instead of finall and then there is no need for [0] in the following line. best,',\n",
       " 'Excellent , thank you.   Content, instruction  and enthusiasm terrific',\n",
       " \"Pretty much new to code other than html & css, I've dabbled and failed, but I'm going to give this a try, this is the best scraping tut I've found. For a long time I've wanted to build a very basic comparison site for my hobby because people spend silly amounts of time searching for the best prices and so far nothing exists to help us, most retailers don't have API's so scraping is the way to go. Wish me luck!\",\n",
       " 'Hello,\\nFirstly, I would like to thank you for your great work. Then, I have questions hope you can help me. \\nCan you guide me how to get the deep links?\\nE.g; if you click on the items (like in 6:32 ) then there will be some links, and how to get that links? \\nAnd how to write code recognize a link structure for crawler \\nlike: http://abcd.com/asas/asdasd/adasdas/123/dfdss ......... and I only want to get \"123\". I hope you can understand my questions.',\n",
       " \"This is actually the coolest thing I've seen in my entire life. Wow. Thank you so much I love you man.\",\n",
       " 'I love how passionate you are about this and how clear it is to understand your explanation. I WANTS IT ALL! GIMME! Lol Awesome video!',\n",
       " 'I successfully got my version of this to run. I have a couple of questions. \\n\\n1) Do you need to specify the [0] index for each variable in the for loop? I just tested with and without on a sample script and it made no difference.\\n\\n2) I don\\'t have Excel, so I imported the CSV into Google Sheets. I am getting a \"-\" showing up from what I believe is the item price varaible. It is breaking the line in my CSV and moving the price to cell A3 instead of cell C2. I have tried price.replace(\"-\", \" \") (price is my chosen variable) and it is not fixing the issue, nor removing the \"-\".',\n",
       " 'Loved the video! I can’t tell you how much it helped me.\\n\\nI am a total noob to all this in general, so I apologize if the question is silly.\\n\\nHow did you get the program to run so the data went to excel? I keep getting a syntax error',\n",
       " 'Nice video, \\ncan you please teach us looping through multiple pages or urls',\n",
       " 'Very good explanation. Thanks! You have good teaching skills!',\n",
       " '16:04 Command for it on windows is  CTRL + SHIFT + P :)',\n",
       " \"This was very good. I'm a beginner to Python and this webscraping tutorial left me with very little questions.\",\n",
       " 'Really really helpful! Thank you so much!',\n",
       " 'Thanks for the video!\\nYou made our day by sharing knowledge! I learned with my friend and she too says Thank you!',\n",
       " 'this is gold , very informative video and easy to understand',\n",
       " \"Hey! new guy here, so how would you write the 'if' condition in the loop for missing values?\\n\\nThe pseudocode I can think of is - if variable = blank then interpret variable as python None -\",\n",
       " 'You explained amazingly, keep creating more content',\n",
       " 'Wish you would make more content like this . There are other youtubers like codeforfreeacademy and the big ones but would love to find more perspectives from other average people however i very much enjoy this content and got to learn a few things I did not know.',\n",
       " 'Wonderful, saved us a lot of time. Amazing!',\n",
       " \"Thank you so much! This was so easy to follow, you've earned a subscriber!\",\n",
       " 'That was very helpful in my project.\\r\\nThank you!',\n",
       " 'could you upload the script?',\n",
       " \"Thanks a lot for this video, I have been searching on internet for web scraping programs on python but I couldn't find one that worked ! Your video is perfect man !! I just have a question on a detail in the csv file: how do i do to sort the elements in different columns like yours because it's putting everything in one column. Thanks for you response, and continue making such useful videos !\",\n",
       " 'Great tutorial. Right to the point',\n",
       " 'such an easy to follow tutorial THANK YOU SO MUCH God Bless!!!!',\n",
       " 'Amazing tutorial! Thank you!',\n",
       " 'This is very helpful.  You make this technical information easy to understand.  Thank you very much.',\n",
       " 'When I watched this tutorial, it seemed easy to scrap until I stuck a thousand times while actually scrapping a webpage. Happy Coding for dummies lollll',\n",
       " 'Damn your tutorial was very good. Helped me make a CSV for all apartments in the area i want to move that accepts dogs. As a side note I had a problem with the open file, if it gives you error about encoding or something about char add this to the \"open()\": open(filename, \"w\", encoding=\\'utf-8\\') . You need the encoding part and its all good.',\n",
       " 'Great tutorial! I just made a Jupyter notebook while following it, feel free to improve upon (for example more explanations as the ones given in the video): https://github.com/thepartisan101/NLP/blob/master/Web%20Scraping%20with%20bs4.ipynb',\n",
       " 'That was a great tutorial. Thanks so much!!!',\n",
       " 'Really appreciate this tutorial, had to make some minor tweaks to keep it current - but the end result was the same! Thanks in 2020',\n",
       " 'Thank you very much Sir for creating such an informative video tutorail.',\n",
       " 'Amazing tutorial thanks for your help',\n",
       " 'I\\'m getting this error when I try to run it:  \\nFile \"<stdin>\", line 2, in <module> \\nNameError: name \\'page\\' is not defined',\n",
       " 'Great video!\\n\\nI  am having errors when I look for the brand. I even used the code avalaible in the description and still gave me the same error. Further analysis of the container, I saw that the \"class = item-branding\"<a href>  division (the one that contains the img) was not included when I the page_soup.find_all(\"div\", {\"class\\': \\'item-containers\\'} . I notice by printing container in the for loop. The error that it gives me is:\\n\\n\"ResultSet object has no attribute \\'%s\\'. You\\'re probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\" % key\\r\\nAttributeError: ResultSet object has no attribute \\'img\\'. You\\'re probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\\n\\n\\nNeed help :)\\nThanks',\n",
       " 'the best and detailed  web scraping video i have ever seen. Enjoyed a lot !',\n",
       " 'Excellent tutorial. Thank you so much.\\nI profite to ask for helping: when you are on amazon of 1 products page , and you want to scrap page 2 , 3, 4, etc, is there a code to scrap page 2, 3, 4, etc?? or do you have to scrape one by one?... thank you for your help!',\n",
       " 'Its awesome  tutorial bro. scrapping is a difficult task for me untill i watch this video .thanks a lot.and i request to do a video on pipelines in data science if possible,',\n",
       " 'Loved the tutorial. Thank you ',\n",
       " 'you look like a god when your writing multiple lines at the same time.',\n",
       " 'I don’t usually comment on videos but this was phenomenal. Thank you.',\n",
       " 'loved it, though i m very beginner in data science and have zero knowledge in it, i watched the entire video and tried to grab everything possible discussed here',\n",
       " 'thank you so much this is exactly what I was looking for my company. i dont think I have the time and skills to do it (even if you explain all of it) but at least I understand now what I can ask from someone competent in this.',\n",
       " \"It's very educative. Perfect job! Thanx a lot!!!\",\n",
       " 'Hi Phuc, this is great - thank you! What if a page requires credentials> is there a way to program that in?',\n",
       " 'I get an error when trying to call uClient & page_html. It says: File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\", line 1318, in do_open\" and then like 40 more lines like that but from different files like client.py and ss1.py and so forth. I don\\'t really know how to fix it. Anyone here has the same issue?',\n",
       " \"That was great!  You're a great teacher.\",\n",
       " 'I did a Web Scraper not so long ago with another set of tools. This video has motivated to create one, too!',\n",
       " 'Easy to follow and understand . Thank you',\n",
       " 'AWESOME TUTORIAL! Has all the information needed to start web scraping',\n",
       " 'Excellent presentation, well done!',\n",
       " \"You don't want Annaconda. There's so much junk that comes with it. Just install regular Python and open your terminal or powershell and install packages you need. For example, pip install beautifulsoup4, if you want to install Beautiful Soup. It's MUCH more cleaner than installing annaconda rubbish.\",\n",
       " 'Great tutorial thank you. Why do I sometimes not receive any containers when I run the script? Is this an issue with late JS loading?',\n",
       " 'Excellent, the video is really very amusing and most importantly the way you code is very excellent.\\nI just fest that coding is very joyous if we can code the way u are doing.\\nReally liked it and thank u a lot.',\n",
       " 'it must have been a magic day when I saw this for the first time 1.5 years ago !!! its where i all started !!! Thanks! Best Video & Intro into webscraping for absolute begginers !! Thanks (notable mentions to Corey Shafer who I was watching a a few weeks earlier, who gave me the taste of it & how easy it could be to use/do). Thank you friends!! An amazing tool!!',\n",
       " 'This is gold for someone learning python and seeing its application.',\n",
       " 'Great tutorial, thank you so much !',\n",
       " 'the first div that it showed was item badges how do i navigate to different divs?',\n",
       " \"I didn't like it...I loved it\\nClear and concise ️\",\n",
       " 'very informative, short and sweet! Thank you.',\n",
       " \"I don't have any words to explain how much this video was helpful. Hope soon I will use this feature.\",\n",
       " 'Fantastic Tutorial!\\n***If your findAll is returning empty in some cases, use a try, except case: Example.... \\ntry: containers.findAll  except Exception as e: container = None',\n",
       " 'We want more data scrapping video! This was awesome!',\n",
       " 'Hi, I\\'m getting stuck at 28:50 when running the script.  How do I solve this problem?\\n\\n$ python Dojo.py\\nTraceback (most recent call last) :\\nFile \"Dojo.py\", line 18, in <module>\\n\\n    brand = container.div.img[\"title\"]\\nTypeError: \\'NoneType\\' object is not SUBSCRIPTABLE\\n\\n\\nBest Regards',\n",
       " 'awesome!. had to make my way around a couple of modifications of the website but really good!.  just finished my first web scrap!!!',\n",
       " \"When looking to explore what is going on in the containers I had issues with the unicode encode error (I am a n00b). Using     print(container.encode('utf-8'))   would help you see what is in your elements as you work through prototyping!\",\n",
       " '¿Puedes publicar un video sobre cómo scraping web con Octoparse?  Creo que esta herramienta es más adecuada para principiantes y no requiere python  https://www.octoparse.es/',\n",
       " \"Thanks for this. I couldn't be bothered to create colour schemes by hand for an application I am writing so I wrote a program to scrape them from color-hex.com! Saved so much time and effort designing.\",\n",
       " 'very helpful but I was wondering how you could clear words after a certain phrase. For example in the video you used a method to wipe out the spaces before and after, what if I wanted to wipe out a whole phrase and only took a selected keyword?',\n",
       " 'when I try to follow, it gives me the following error message: \\nbrand = container.div.div.a.img[\"title\"]\\nAttributeError: \\'NoneType\\' object has no attribute \\'a\\'',\n",
       " 'This is absolutely the best and most updated tutorial of web scraping.',\n",
       " 'This is way better made than the newer videos in R. Can you please make an new video like this expanding on the concept? Perhaps a script that scraps the data then turns it into a new html document?',\n",
       " \"It's really beautiful and amazing. I have learnt many things in a single video. It's like different fruits in a single banch of a tree. Keep it up. Big clap for you.\",\n",
       " '+1 for the short and straightforward tutorial.',\n",
       " 'Thank you very much, this was a great and useful video!',\n",
       " 'brand = container.find(\"a\", {\"class\":\"item-brand\"}).img.get(\\'title\\')\\n\\nyour welcome',\n",
       " 'HONESTLY YOUR EXPLANATION WAS PERFECT! THANK YOU',\n",
       " 'Awesome video! I think you can use join() to concatenation with delineator',\n",
       " 'I am a beginner in Python. This was awesome.',\n",
       " 'Thank you for this video. You are very good at what you do.',\n",
       " 'Excellent work!  Where can I find the code please?  Thanks a lot!!!!',\n",
       " 'brand = make_rating_sp[0].img[\"title\"].title()\\nTypeError: \\'NoneType\\' object is not subscriptable\\n[Finished in 3.074s]\\nanyone know why this is happening? or how to fix this?',\n",
       " 'Beautiful - thank you so much!!',\n",
       " 'Excellent tutorial. Thank you!',\n",
       " \"Great video! if you are watching in 2020 and using BS4, use the get() method to capture data from different fields within the <a block e.g. container.get('img')\",\n",
       " 'Thank you for sharing this video.. Awesome content. I like the way you explained the flow and hands-on. Here is a question, when I tried scrapping other site , I am getting 403 forbidden error , how do I fix that? Is it possible to scrap a secure site?The website has https.Another question I have is,  in a real world situation, do data scientists go through each individual websites and web scrap like you did or there is some other ways? Lets say I want to get a list of all top sales online in women clothes, then do I web scrap all the websites selling women clothes?',\n",
       " 'I would really like to get into a bit of data collection to do some buying and selling of tcg cards.\\nSee how often a card is mentioned in videos and how it relates to card reveals and how prices change.\\nCombine it into an algorithm added with some small manual inputs for insights beyond data, like how big its potential is due to an effect relation to another card.\\n\\nCould make me some good money.\\nThe market is definitely easy to predict, it is only a matter of having enough data to do predictions.\\n\\nThe only reasons I didn’t want to engage in such attempts is, that this would increase card prices for everyone and drain money from the community, whilst also having a carbon footprint for shipping/reshipping of cards. So morally I don’t like the idea too much, but it is an exciting idea.',\n",
       " 'when i type uclient = ureq(my_url) it gives me a 403 error forbidden and a bunch of timeout, does this mean that it works but it crashed or will crash if it runs?',\n",
       " 'Wowowow awesome video, I thought learning web scraping might take me a week or so, and thanks to you I learned it in 30 minutes',\n",
       " 'Amazing!! Thanks so much for the tutorial!',\n",
       " 'Awesome  video. Can you do a Python program to scrape current stock prices from yahoo finance for a list of ticker symbols in a csv or xlsx  file?',\n",
       " 'I expected this video to take me 30 minutes to do, because it takes 30 minutes. 10 hours later I HAVE MY FIRST WEBSCRAPER THANK YOU VERY MUCH! I still did not manage to get it to be an csv, but made a .txt and it is fine for now. Thank you so much again! tutorial from dataquest.io came in very handy as well!',\n",
       " 'Excellent video!  Thank you for your help!',\n",
       " 'Can anyone say what is the syntax when you are using two objects inside findAll. \\nIf the HTML is    <div class=\"xxxxxx\" id=\"yyyyyyy\">',\n",
       " 'I was searching for a selenium alternative, this is what i was expecting to find. A webscraper which runs in background. Thanks for the tutorial really usefull!',\n",
       " 'Suggestion:\\nFrom 28.11.2020: When I tried to replicate the code, I noticed that the website now contains \"ads\"- which are basically graphic cards sold by other sites. The problem occurs when setting the brand name which in the video is:    \\n\\n brand = contain.div.div.a.img[\"title\"]\\n\\nFor the ads, divs are ordered differently. My workaround takes the full title of the graphic card (Which is the same for ads and regular entries) and extracts only the first Word, which fortunatly is always the brand name:\\n\\nbrand = container.find(\"img\")[\"alt\"].split()[0]\\n\\nCheers, and thanks a lot for the great guide!',\n",
       " 'Thank you, great confidence booster in web scraping.',\n",
       " \"That's really wonderful. Thank you very much for this video. I'll try it.\",\n",
       " 'GR8 tutorial!  Could you do a tutorial on how to extract tax and shipping cost data of a product for different shipping destinations? Thanks!!!',\n",
       " 'shipping_container = container.findAll(\"li\",{\"class\":\"price-ship\"})\\n\\nGETTING THIS ERROR\\n\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\nTypeError: \\'tuple\\' object is not callable',\n",
       " 'this is really helpful very much for a free tutorial. thank you so much',\n",
       " 'Thank you so much for this good sir, this was excellent!',\n",
       " 'Loving your coding skills. Was just about giving up on Web Scraping. Then BOOM!!! I found this. :)',\n",
       " 'Very well explained. Thank you ',\n",
       " 'hey, great video!\\nI got a bit confused on the part with the \"for loop\". in the video, you wrote:\\nfor container in containers:\\n    brand = container.div.div.a.img[\"title\"]\\n\\nand further on the video you added similiar statements to the one above.\\nmy question is, how does python knows to search and activate on the \"container\"? I mean, it\\'s inside the variable (for example, brand). And also, \"container\" on the sublime text haven\\'t been configured.',\n",
       " 'this soup is very beautiful, goddamn',\n",
       " 'This is a great video, thank you so much, I followed that step by step.',\n",
       " \"Awesome video! I can't wait to try this out and get it into a SQL DB\",\n",
       " 'Great tutorial!  How about setting up a script to run automatically, changing just one parameter with each pass?  Of course, I think I have to set up one of my laptops as a server first?',\n",
       " 'Amazing tutorial my friend. Thank you so much!',\n",
       " 'Great introduction to web-scraping. Thanks for posting this.',\n",
       " 'I keep getting 0 when I call len(containers)',\n",
       " 'Loved it! A good tutorial finally!',\n",
       " 'Hi, this is really cool! Absolute Legend :D \\nMy_request > I would love to see a tutorial on how to scrape hotel prices for London for example. I would also like to know how to loop through dates in order to show seasonality patterns in the data.',\n",
       " 'Thank you! Great for beginners',\n",
       " 'Hi, loved this tutorial but is there a way to automate the web scraper. For example how can i use it to scrape  data from a site every 24 hours assuming the content on the site changes every 24 hours',\n",
       " 'Thank you for the great video!\\nI followed your instructions, but by checking the functionality of the script I get an error:\\n\"ImportError: No module named bs4\" I don´t know why because the installation was successful. The import in the terminal was also successful. Does anybody have an idea?',\n",
       " 'def Data Science Dojo():\\n    Data Science Dojo = (\"like\", \"share\", \"sub\")\\n    good job = (input.comment(\"Thanks you very much ! \"))\\nif good job in Data Science Dojo :\\n\\n    print(\"love and respect from Kuwait\")\\nelse:\\n    print(\"sorry maybe next time\")\\n\\n\\n Data Science Dojo()\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n-------\\n\\n\\nOutput :-\\npeace out and happy basic coding :D',\n",
       " 'Just one question: we are pulling data from all the items in the same page. What if there are others for the same list and in subsequent pages? Can we pull them also using url extensions?\\nThanks in advance',\n",
       " 'Yes Sir, learned a lot. Thank you very very much.',\n",
       " 'In case anyone gets stuck at the container.div part, other comments have addressed this but here is a solution that is current. instead do divWithInfo = containers[0].find(\"div\",\"item-info\"). This will create the object you need. Afterwards you can work you\\'re way toward the goodies with the following example: divWithInfo.div.a.img[\\'title\\']',\n",
       " \"thanks for putting together, I didn't run in anaconda, just regular python 3.7  and it worked fine\",\n",
       " 'Tips: add the headers paramter for fake web client to the Request call function to avoid http 403 or 503 error when requesting some websites with security against spider/bot.\\nhttps://stackoverflow.com/questions/44280807/data-scraping-using-python',\n",
       " \"I'm on ubuntu, is that ok?\",\n",
       " 'Man, you just saved my Python programming subject. Thank you alot',\n",
       " 'First tutorial, worked with web scraping and uses classes. # Big Thanks!\\n\\nfor UK users: shipping caused error for me, so used the price tag instead\\nprice_container = container.findAll(\"li\", {\"class\":\"price-current\"})\\nprice = price_container[0].text[0:7] # [0:7] to get the range of it',\n",
       " \"your such a great teacher! Just because you can code doesn't mean you can teach. Awesome!\",\n",
       " 'Thank you #datasciencedojo so much for this great tutorial. I have got a question - I want to click a button \"load more products\" and then scrape more. How do we do that?\\nI would really appreciate any pointers.',\n",
       " 'Its better to monitor xhr response than webpages. Many websites will render information and containers later. If the websites use JSON internally then it becomes far easier to parse that instead of xpathing.',\n",
       " 'Just use pycharm, man :-P',\n",
       " 'Really excellent way of teaching...understood every single word..just one doubt...what if i want to extract historical data ??say i want to scrape through  last 1 month airline price details  ??Appreciate if you could point me in the right direction in this case.',\n",
       " 'Below is also a great article on Web Scraping using Python & Beautiful Soup and what we can do with scrapped data. Complete Source Code is also provided.\\nhttps://www.opencodez.com/web-development/web-scraping-using-beautiful-soup-part-1.htm',\n",
       " 'Thank you so much for this video. This really helped me to go in the right direction. :)',\n",
       " 'Great video u explain very well plz upload more python related videos',\n",
       " 'Your video was really helpful, thanks a lot!',\n",
       " 'Awesome,learn a a lot from one short tutorial !!!',\n",
       " 'Good training video. Pleased to see you added saving to a  csv.',\n",
       " 'Thanks bro! very useful and well explained !',\n",
       " 'that was really fantastic, quick and usefull',\n",
       " 'I enjoyed this too much. Learned something now today! thank you so much.',\n",
       " 'Great video! Please do more video about web scraping with python',\n",
       " 'This is a great video. Thanks so much. How about how to scrape and download images from a website?',\n",
       " 'Very useful tutorial thank you for sharing knowledge',\n",
       " 'Thanks for the video. This was the best web scraping tutorial I have seen on youtube.',\n",
       " \"Awesome... Great video... Learned loads that other videos and written tutorials don't cover... \\\\o/\",\n",
       " 'Really useful. Thank you very much!Please carry on',\n",
       " 'Just heard of web scraping today, and this video made total sense on how it works. ty!',\n",
       " 'You are sooooo comfortable to listen to. Not because you have a perfect pronanciation and a seamless script you are gliding through. You are just talking but not constantly jumping back and forwards. Accurate tempo and personality in your voice.\\n\\nNew subscripion',\n",
       " 'The first video of this type that really made sense to me ...   thank you very much.',\n",
       " \"Ho yes Thank you!! I'm a really beginner in python and I almost undertood everything!! I already have many idea how to use that ;) Do you know how to scrape pages on a website where you have to login in? I mean is it possible to login in on a website with python? Again Thank you so much :)\",\n",
       " 'such an amazing video, thank you so much!',\n",
       " 'Fantastic video! More data science videos! Im an aspiring data engineer',\n",
       " 'Thank you for the video. I followed along but ran into issues when attempting this on another site. I found a dataset of containers I wanted to use, but when I ran len(containers) it returned 0. This was trying to use findAll. I tried to to page_soup.div.div.div and navigate down to the containers with no such luck. I was wondering what I would need to do different on this site: https://www.twitch.tv/speedgaming/videos?filter=archives&sort=time',\n",
       " \"It's very helpful. Thank you!\",\n",
       " 'YOU EXPLAIN EVERYTHING SO WELL!!!!!',\n",
       " 'Great video bro... Finally an explanation I can understand.',\n",
       " 'two days finding answers and you give me all that i need in 30 minutes !!! thanks so much!!!',\n",
       " ' Straight to the point! Very nice!',\n",
       " 'Wow you such a way of teaching that just made something intimidating look so easy. Thank you sir XOXO.',\n",
       " \"Great tutorial. Wish you've used Jupyter Notebook instead of the command line. It would be easier to follow along.\",\n",
       " 'I usually don\\'t comment but this video is really awesome and I really learned a lot.\\nI also have a question: Can\\'t I use \"REGEX\" to get to the details of the product',\n",
       " 'Extremely helpful video!! \\n\\n\\nWould it be possible to make this program repeat every \"x\" time interval (ex. every 5 seconds) and write over the old document? where would someone start who was looking to do this? Basically every x interval it would be looking for new products. If a new graphics card came to the list, it would then update.\\n\\n\\nHope this makes some sense',\n",
       " \"If I wanted to print all of the say results for the item titles, how would I go about doing that in the for loop? Since obviously, I can't increment a string, and if I do 'containers[0].text' it will just print the same thing 12 times.\",\n",
       " 'Awesome! Thank you!\\nPlease, more videos about scraping.)',\n",
       " 'Thanks for your video, can you make one doing scraping using proxycrawl service?',\n",
       " 'Hi, I have the following issues: 1) The output in the command line has all results , but in the excel I have 1 row only. 2) The shipping data is going to incorrect column - it is shown on the second row in the column \"brand\". The row for output format that I have is the same like in the video:   f.write(brand  + \",\" + product_name.replace(\",\", \"|\") + \",\" + shipping  + \"\\\\n\")\\nf.close()',\n",
       " 'you guys should really continue this series',\n",
       " 'Excellent Video! Please keep making such content.',\n",
       " 'It was really great video, I am python beginner and it helped me a lot. But I would like to know how to scrap google reviews of places. It will be great if you post some video on that. Will be really appreciated.',\n",
       " 'How can I use the second item of an HTML typer after I defined the container (20:00). On the site I am scraping on there are three H4, I need the second one but also need to get one layer deeper afterwards.... Any ideas?',\n",
       " 'This is awesome. Thank you for sharing :)',\n",
       " 'Thank you! Very informative.',\n",
       " 'What a clear  explaining,    thank you  man,,, i m learning... do you teaching on Udemy ?',\n",
       " \"Nice tutorial, thank you very much. Do you or someone else in the comments know how to put in browser headers, so that the website (Amazon) isn't blocking my requests?\",\n",
       " 'very nice explanation, it is very useful to me',\n",
       " 'Great tutorial indeed. Thanks',\n",
       " 'Amazing tutorial!!!!\\n\\nThanks :)',\n",
       " 'Thanks for the tutorial really helpful',\n",
       " 'very useful, thanks so much !',\n",
       " 'Awesome tutorial thank you. but for a non-tech user it is quiet hard to do a workable scraper for my WooCommerce store. As a side solution i am using eCommerce scraper via \"ESCRAPER\" maybe it helps somebody too.\\nBut I am not giving up))) Thank you for your input!!!',\n",
       " \"It's like magic. Thank you so much for this video.\",\n",
       " 'Good job man, very helpful',\n",
       " 'Tip: use SelectorGadget to find the css selectors you want',\n",
       " 'Great video! How can I change the search inputs on a website when I shop for something, so it searches the website with my input and get me the data? Please help ',\n",
       " 'Excellent Explain as compare to other. Hope through this channel , my concept on Data science will clear shortly. And only this channel where i got good concept on R .',\n",
       " 'Hello, with my project that I am working on, there are only <div> tags, so it makes navigating the tags difficult. The data that I want is located between two <span> tags, and I am unsure of how to collect that data. Once again, after I write ```container.div.div``` it cannot go any farther into the HTML code, despite there being more <div> tags embedded in it. any help or resources would be greatly appreciated.',\n",
       " 'Thank you so much, this really helped me!',\n",
       " 'You get down Phuc! Appreicate you sharing. More Web Scaping videos please where you also post your code - GitHub / Jupyter Notebook status.',\n",
       " 'great video, I am not sure how you loop multiple pages, you said something about it.  How do we loop over multiple pages, any one can give me a hint that would be great. thanks',\n",
       " 'its really perfect tutorial, thank you',\n",
       " 'u are very good explaining this content, thank you so much. it so easy :D',\n",
       " 'Amazing, i was trying to do exactly that!!',\n",
       " 'Great tutorial, thanks!',\n",
       " 'Wow! Amazing trick. Kindly upload more like this.',\n",
       " 'Hi, how can we do it for multiple pages? Suppose what if there are like a 10-20 pages ?',\n",
       " 'You shouldn\\'t use names of variables in \"camel case\". Use \"snake case\" for it.\\nProgrammers use \"camel case\" for classes naming.',\n",
       " 'Here is my version of code. I used requests instead of urlib.requests since they are much easier to use.  I scraped newegg\\'s laptops.\\n\\nfrom bs4 import BeautifulSoup as soup\\r\\nimport requests\\r\\n \\nf = open(\"data.csv\", \"w\")\\r\\nf.write(\"Brand, Title, Shipping\\\\n\")\\r\\nurl = requests.get(\\r\\n    \\'https://www.newegg.com/Laptops-Notebooks/Category/ID-223\\')\\r\\nhtml = url.text\\r\\npage_soup = soup(html, \"html.parser\")\\r\\ncontainers = page_soup.findAll(\"div\", {\"class\": \"item-container\"})\\r\\nfor container in containers:\\r\\n    brand = container.div.div.a.img[\"title\"]\\r\\n \\n    title = container.find(\"a\", {\"class\": \"item-title\"}).text\\r\\n \\n    shipping = container.find(\"li\", {\"class\": \"price-ship\"}).text.strip()\\r\\n    f.write(brand + \",\" + title.replace(\",\" , \";\") + \",\" + shipping + \"\\\\n\")\\r\\n \\nf.close()',\n",
       " \"If you're using Mac and you've installed Python3, make sure to double click the Install Certificates.command file before you use the uReq func or you may have SSL Certificate verification errors. \\n\\nFile Path: Applications > Python 3 > Install Certificates.command\",\n",
       " 'well presented !\\n[1]  if code could be downloaded, it would be wonderful. \\n[2]  if Chinese character be read as it is read on the screen.  If not, what codes be added.',\n",
       " 'Omg  You are the best! I have finally started to understand everything! Millions of thanks ',\n",
       " 'OK but... at 24:40 this came very close to what I need for this project!\\nWhat I need now is something like: container = page_soup.findAll(\\'a\\', {\"class\":\"set\"}.[\"Change\"] (or something)',\n",
       " 'Thank you for the video, very clear. Someone can tell me how to avoid being banned from some websites because of daily scraping ?',\n",
       " 'Thank you for that amazing tutorial.\\nWhat if the web developers changed the classes names? Than I have to rewrite my script or there is an easier way?',\n",
       " 'Senior Data Scientist, Senior Database Engineer... I know a fellow gamer when I see one!  Thx for the the Tutorial.  All this time...all I ever wanted from most of the internet was the ability to \"scrape\" (new term for me) what I wanted so that I can do something with that data.  I like to organize things and categorize them.  I always thought rss was okay...twitter okay...reddit okay...but I just want specific feeds from those sites and this is exactly what I was looking for!  Better than paying a monthly fee to somebody who won\\'t even teach you how to do it.  Maybe its from collecting cards as a kid or playing video games that had really in depth inventory systems (rpgs).  But it is enjoyable when you can get the exact bit of information you want and then do something cool with it.  This is helpful!  Where were you when I needed to organize my bank in world of warcraft!!!',\n",
       " 'If you get a \"403 forbidden\", add a header to your variable like so : \\n\\nurl_1 = \\'Your url here\\' \\nurl_2 = Request(url_1,headers={\\'user-agent\\': \\'Mozilla/5.0\\'})',\n",
       " 'Great video! Really enjoyed it. \\nDo you have any suggestions on how to web scrape data from Facebook? Is that even a thing?',\n",
       " 'Very helpful!! Thanks :)',\n",
       " 'That was so well done! \\nI like to learn Selenium from this guy.',\n",
       " 'Very good stuff! Thank you!',\n",
       " \"thank you so much for this tutorial. I enjoyed the tutorial a lot until the last part where the task was to open saved python text file... It's showing that there is no such file on directory... could you please help me out with the error that I'm dealing with???\",\n",
       " 'So thankful for this, I was able to run it and scrape similar information off of a coding website. I had some trouble with installing BS4. Tip, I used pip3 to install BS4 to keep everything clean. \\n\\n\\nsudo pip3 install bs4',\n",
       " \"Hey dude great video... wanted to scrape keyword so that when we search any keyword (example : Company name) it will go on google and extract basic info such as company's -URL,  contact details, latest news, social media page etc... can you make a project of such kind? and is there anyone that can help?\",\n",
       " 'Thanks for a great video! \\nIt looks like not every website allows scrapping, how do I know if it does? Check robot.txt file? Not sure what to look for',\n",
       " 'I think we can also use the prettify() function instead of going to jsbeautiful.',\n",
       " 'Sir, you got me have a real taste of python.. <3  Thank you so much..',\n",
       " 'Hands down one of the best CS tutorials on this god forsaken website. I will subscribe and continue to watch :D',\n",
       " 'Thnx for the beautiful video this is help me a lot even i am b.com student but with the help of this video i did what i want.. Thnku so much bro..... ️',\n",
       " \"I found this in 2019 and it's awesome! \\nthank you <3\",\n",
       " 'thank you so much. super helpful!',\n",
       " 'I have a question. I followed you all along. But there are other divs which have a class of item-container also that are located at the top. They are within a div that has an id of recommendations. And when I do containers[0] what is shown is not the first product that has bigger image, instead what is shown is the first product at the very top that has smaller image--the products that are recommended? Please help. Thanks.',\n",
       " 'One of the best scraping tutorials good job',\n",
       " 'Pretty helpful, thanks alot dude',\n",
       " 'Excellent tutorial!',\n",
       " 'this was so much easier than i thought, thank u',\n",
       " 'To bypass SSL error you might get on MacOS, use this piece of code: \\n\\n\\n\\nimport certifi\\nimport ssl\\n\\n\\nuClient = uReq(my_url, context=ssl.create_default_context(cafile=certifi.where())) \\n\\n\\n\\ninstead of the simple \\n\\n\\n\\nuClient = uReq(my_url)\\n\\n\\neverything will work fine.',\n",
       " 'Awesome! Thank you for this video.',\n",
       " 'Can you please make a video about actual code needed to iterate thru all pages of the web site?',\n",
       " 'Hello! I was trying to follow your steps. But when i put the page_soup.h1, it is not getting the information from the webpage. Can you please tell what could be wrong?',\n",
       " \"you may want to change the parser type 'html_parser' to 'html.parser'\",\n",
       " 'VERY CLEAR!!!thank you',\n",
       " 'Thanks for this awesome tutorial.',\n",
       " 'Amazing Intro video for scraping from the web!',\n",
       " 'Excellent! Thanks a lot!',\n",
       " 'Amazing, thanks for sharing!',\n",
       " 'I am from commerce background. I have zero knowledge of all the programming language. I found your video and explanation so good that at least now I can start my journey into scrapping and coding. I am so thankful at the moment. Love your channel. Thank you so much.',\n",
       " 'great video, learnt it so fast',\n",
       " 'Great video! Is there a list of other videos done by this guy?',\n",
       " 'Thanks man it helped me a lot!!',\n",
       " '\"w\" - Write - will overwrite any existing content\\n\\nf = open(out_filename, \"w\") \\n\\nand inside the loop we said\\n\\nf.write(brand + \", \" + etc...)\\n\\nwouldn\\'t it overwrite the file every time an iteration is completed?',\n",
       " 'Hello \\nExcellent video I have been following all the steps you have done but when I got to open a second command line to run the python file it gave me this error (python my_first_webscrape.py\\n  File \"my_first_webscrape.py\", line 1\\n    from urllib.request import urlopen as ureq\\n    ^\\nIndentationError: unexpected indent)',\n",
       " 'Awesome video. Ran into one problem, all my prices are numbers, none of them show up as Free Shipping even though when I look at the page source and can see that they say Free Shipping.',\n",
       " 'awesome video. keep doing the great work.',\n",
       " 'You helped me a lot <3',\n",
       " '️love. This is my first ever seen video on web scraping and you did an awesome job explaining it to me..',\n",
       " \"Thanks for the nice tutorial! Why I can't code that in an IDE, just in a terminal?\",\n",
       " 'Excellent, very clear',\n",
       " 'Saw many videos on web scraping but yours was probably the best one.',\n",
       " 'Really enjoyed the video. Very helpful.',\n",
       " 'Beautiful, clean, right speen and consise contenet , thank you very  much',\n",
       " 'Loved this very clear tutorial. You should look into using Jupyter Notebook instead of having to jump between the script and the command line. Thanks',\n",
       " 'Boy is this ever clear. Very straighforward presentation!',\n",
       " 'I enjoyed this video A lot I also Liked how much you explained it',\n",
       " \"Wouldn't it save some time to select the required tag using: Inspect > Right-click on tag > Copy > Selector/Xpath ? Just wondering\",\n",
       " 'This is just awesome video. Thank you very much.',\n",
       " 'While following along step-by-step (in python shell); I get an error when I run the line that says \"uClient = uReq(my_url).   I can\\'t seem to get past that point.  I get a message that says \"Traceback (most recent call last):\" and then proceeds with line after line of what looks like commands from the urllib.  I\\'m new to this, so any help in pointing me in the right direction of what might be happening would be appreciated.',\n",
       " 'That is awesome. Thank, man\\nit was really helpful',\n",
       " 'Really nice tutorial!',\n",
       " \"Let's say we're working with a really sloppy site where the developer can't handle their own naming conventions properly.\\n\\nI've got two separate arrays set up - one for table headers, one for dynamic content.\\n\\nBut, due to the genius coding on the site, some of the headers show up as dynamic content.  The bogus data does have a unique ID, but the problem is it meets the wildcard findAll for both array creation lines that I've made.\\n\\nie: \\n\\nGood data1:  fooBar_[random characters for each item]_lblThisRow\\nbad Data1: fooBar_[even more random characters]_lblSillyProgrammerUsedWrongPrefix\\nGood data2: fooBar_[more random characters]_lblThatRow\\n\\nIs there any way I can create a few lines of code to pluck out the few instances of bad data?  The (bad) naming convention is at least consistent with the template, so any search results come back have the same mistake.\",\n",
       " 'Good Video. Question.  How do you do it if the website checks if you have javascript enabled?  Can you make a tutorial on how to do it? Take example:  www.marketwatch.com',\n",
       " 'I really enjoyed it. Thank you.',\n",
       " 'Really nice video my friend! Thanks a lot',\n",
       " 'thanks dude , that was a really nice tutorial  (Y)',\n",
       " \"Thank you! If someone is scraping another website that won't give permission, check here and try using the correct answer: https://stackoverflow.com/questions/44865673/access-denied-while-scraping\",\n",
       " 'Thank you for great content!',\n",
       " \"This is ridiculously awesome bro! Love your energy. I def appreciate you making this video. I'm addicted to you channel now.\",\n",
       " 'The best tutorial. Thank you. Much better than all videos in russian lang',\n",
       " 'Yes, enjoyed and learnt a lot. Thankyou.',\n",
       " 'Hello! I\\'m trying to scrap a website to collect video links I\\'ve followed upto 14:30 of video. i\\'m having difficulties in using the findAll() or find_all() methords.\\nInside the html there are these  \\n       <div _ngcontent-c8 class = \"class name example\" ......> \\nand \\n       <a _ngcontent-c8 class = \"class name example\"......> \\n\\n\\ntags so if i try to use the \\n\\n\\ncontainer = page_soup.findAll(\"div\",{\"_ngcontent-c8 class\":\"class name example\"})\\n or\\ncontainer = page_soup.findAll(\"div\",{\"class\":\"class name example\"})\\n\\n\\nit returns me an empty list. i\\'ve tried using page_soup.find_all() tags as well but it all returns the same empty list.\\n\\n\\nKINDLY IF ANYONE KNOWS HOW TO RESOLVE THIS PLEASE HELP ME!',\n",
       " '@20:40 If the \"container.div.div.a\" does not work, just do \"container.a\". it works the same.',\n",
       " 'Excellent keep ‘em coming! ',\n",
       " 'Was helpful. thank you!',\n",
       " 'This is great but mine blew up. Do you have the source available somewhere?\\nUpdate: I was scaping a URL that didn\\'t like my \"browser.\" it gave a \\'forbidden\\' error and I thought it was a package error. Now I will watch the video again.\\nAnother update: I followed your steps on a different URL and created a script that will be very helpful to me. THANKS!',\n",
       " 'Awesome video. Very Informative!!',\n",
       " 'Thank you, this was my first web scraping',\n",
       " 'Very nice explained with a very relaxing voice. Thank you',\n",
       " 'Great, that was so helpful',\n",
       " 'I loved this video. Thank you !',\n",
       " 'Excelent content! Thanks!',\n",
       " 'Amazing! Love the content',\n",
       " 'Why do we create the \"containers\" variable? Couldn\\'t we just use \"page_soup\" as the main area to search, since it contains all of the code?',\n",
       " 'Hello.\\nHow do I put the results of web scraping on a website using something like Django?',\n",
       " \"This is probably a foolish question but do anaconda and sublime have to be in the same folder? If anyone could let me know I'd really appreciate. \\nAlso thanks so much for this video Phuc!!\",\n",
       " 'My dude how on earth you have so little subscribers, your content is awesome',\n",
       " 'best web scraping tutorial, i have been ever seen. thanks bro !',\n",
       " \"6:28 - The goold old times where a mid-upper graphics card (GeForce GTX 1070) could be bought under 400$  :')\\n\\nGreat video, thx!\",\n",
       " 'tip for windows 10 (or others)\\nI had to put .\\\\python instead of just python in the cmd, hope it helps!',\n",
       " 'awesome tutorial , upload more Data Science videos .',\n",
       " 'Excelent, I have a question, I need the images, how can i get the images in format JPG?',\n",
       " 'GREAT VIDEO!! I WATCH THIS 2 YEARS AFTER ITS UPLOADED AND STILL RELEVANT!! THANK YOUU!!',\n",
       " 'best video ever! but i got an error in the loop with the name of the brand, the program gave me this error: brand = container.div.div.a.img[\"title\"]\\r\\nTypeError: \\'NoneType\\' object is not subscriptable\\nThe error appears only in the third loop, it gave me the first two brands but it stoped at the third with that message. But I fixed  it by putting a Try / except into the brand name and it run well but it skiped a few brands like \\'\\'cobra computers\\'\\' and \\'\\'Corn electronics\\'\\', so I dont understand why it happens. If someone knows, let me know pls.',\n",
       " 'Thank you so Much.. :) Followed you and Mission Accomplished..\\nFirstly I was getting issue of SSL: not working... on macOS\\nthen this worked for me...\\nimport ssl\\nssl._create_default_https_context = ssl._create_unverified_context\\n\\nrest was working like butter...',\n",
       " 'Got stuck at 21 minutes...\\nI\\'ve noticed that your example is very convenient for scraping and that, in reality, most websites probably are not.\\nFor example, for the website you used, every item (graphics card) has the same container, literally called \"item-container\", so ther are 12 of those.\\nThe site I want to scrape from though, there are 20 items (car lease deals) and 20 different containers, example deal_3273683264, deal_456736420, etc.)\\nGreat video either way, I have learned alot despite being unsuccessful.',\n",
       " 'A big thumbs up man really helpful',\n",
       " 'This is a good tutorial but for me it would be better if you prototype and script on the same screen or just put he script in the description',\n",
       " 'thank you for your video, I have learned so much from your video! Good luck! regards',\n",
       " 'Hi,\\ncould u plz make one video to explain data scraping from different page of same website?\\n\\nThanks in advance.',\n",
       " \"What if you had a series of urls you wanted to scrape in a loop? I'm assuming the first step would be putting them all in a list like urls = ['url', 'url', 'url']. But then I'm not really sure what I would do after that.\",\n",
       " 'thank you  so much for this video .. Actually i am assing. to submit .. and this is a huge help for me . I am a noob .. but this will help me a lot..\\n\\\\',\n",
       " 'Great tutorial!! thanks',\n",
       " '\"See price in cart\" <- can you add it to the cart and get the price there? In Python, I mean.\\nBetter: can you add 999 to the cart and read the error message to know how many they have in stock?',\n",
       " 'for container in containers:\\n    brand = container.div.a.img[\"title\"]\\n    title_container = container.findAll(\"a\", {\"class:\": \"item-title\"})\\n    product_name = title_container[0].text.strip\\n\\n\\nwhen I run this I get an error saying \"IndexError: list index out of range.\" Do you have any idea why this is happening?',\n",
       " 'You should really use the csv module. Saves lots of boilerplate you had to enter when writing your CSV.',\n",
       " \"Wow i didn't know you can scrape like this too. I've been using Turbo Marketer - Instagram Followers Scraper and it helps me scrape some info in instagram.\",\n",
       " 'Top drawer instruction. Excellent',\n",
       " 'Which key combination did you use at 27:45?',\n",
       " 'Thank you so much this was great! Sooo sooo great',\n",
       " 'hello sir the video content is very useful update the video content for the updated webiste now the existing video content is  very time consuming thank you for the useful content',\n",
       " 'Thank you for making this tutorial!\\nReally!',\n",
       " 'Thanks for making it fun!',\n",
       " 'AttributeError: \\'NoneType\\' object has no attribute \\'div\\'  m getting this error while running the code.It would be great help if anyone could tell how to tackle this.\\nP.s-  brand=container.div.div.div.a.img[\"title\"]\\nI have used div 3 times because the title was present after 3 div.Correct me if I m wrong',\n",
       " 'Great video, thank you.',\n",
       " 'dude your a bad ass. i am barely familiar with html and css and want to get more into bots. i am however very new to this area of coding. i think your video was super good and i was able to understand you pretty good for me not knowing shit but i was a little left in the dust a couple times and wish you had a video for super noobies like myself to become familiar with the lingo and programs. just thought i could use notepad for writing code and like what if i want to say for example scrape information from several different pages. example if i wanted to scrape emails or phone numbers from craigslist would i have to literally go into every ad link  that url..im sure theres a way to have the scraper do that work for me correct? either way great job on your video dude your gonna be my go to sensai for tech stuff',\n",
       " 'Thank you very much. It is fantastic... !!!',\n",
       " 'This is amazing!',\n",
       " 'Awesome tutorial',\n",
       " 'Does the command line have to be in a particular folder? Or must it be in the same directory as the IDE and text interprators?',\n",
       " 'In your example, how to loop second, third or fourth page of the website?',\n",
       " 'Whew,  made it through the tutorial. Thanks a lot.',\n",
       " 'Can you also grab the image of a product and store it?',\n",
       " \"Hi, I couldn't get any result after entering page_soup.h1 or page_soup.anything. I've tried doing it in both terminal and jupyter notebook. Does anyone know what's the problem?\",\n",
       " 'My jaw is on the floor O.O \\nI could not imagine that things like this are possible to do with Python or with any other program. \\nThis is awesome. It is so fucking useful. \\nGosh! \\nI want to learn these things. \\nGreat skill. \\nThank you for this video, for me is like eye-opening information. \\nI will subscribe to your channel.  :D',\n",
       " 'Great video, Thank you!',\n",
       " 'Best Web scraping video on youtube! thank you for that',\n",
       " 'What are some common algorithms used in web scraping?',\n",
       " 'Nice! Thanks for this!',\n",
       " 'Awesome tutorial',\n",
       " 'I looked and findAll is exactly the same as find_all, with find_all being naming convention compliant for python 3.',\n",
       " 'Thanks for the tutorial!',\n",
       " 'thanks man this was very useful',\n",
       " 'How did you make this so simple?',\n",
       " \"I've a question and I really appreciate your help\\n\\nI was trying to use urlopen from urllib.request for a website but it ran into an error saying Error 403 forbidden.\\nIs there anyway to get html of that wabpage, by automation?\",\n",
       " 'super cool, why not make more in depth ?',\n",
       " 'Beautiful work',\n",
       " 'Can somebody tell me how we do to apply this to multiple pages ?? \\n\\nIt finishes with: \\n?page=2\\n\\nAnd I want to do it until page 50',\n",
       " 'This is so good. thanks for the video',\n",
       " 'Please make more data scraping tutorials using python',\n",
       " 'Best Scrapping Video ever.  Thank you.',\n",
       " 'Thank you very much for your comprehensive tutorial video. Good job from Vietnam.',\n",
       " 'GREAT TUTORIAL!!',\n",
       " 'Beautiful man.. loved it.',\n",
       " 'very useful really thanks',\n",
       " 'This was mindblowing.',\n",
       " 'Could you show us something similar but with the data ending up in a google sheets file instead of an Excell file.',\n",
       " 'hey man love the video its really helping for school project but does this work in real time',\n",
       " 'Brilliant! Thanks a bunch...',\n",
       " 'Thanks \\n\\nHow do you loop though all pages ? \\nDo you have to find the url patern?',\n",
       " 'This is great thank you',\n",
       " 'one of the best tutorials ever, tbh',\n",
       " 'Quite Good for Beginners. Thx~~~',\n",
       " 'html5lib and html_parser What are the differences? Thanks in advance.',\n",
       " \"Does anyone know how to get the price text?? I did use the findAll() function but I still can't extract the price text data\",\n",
       " 'Is there a reason to use BS4 instead of Scrapy?',\n",
       " \"I learned so much, I can't believe it!!!\",\n",
       " \"is there a way to download a channel playlist with the names of the videos in a txt or xls file (WITHOUT downloading the video itself)?. I believe 'youtube-dl' doesn't do this\",\n",
       " 'Just one word...., Awesome !',\n",
       " 'What if I want to scrape multiple pages of graphics card? How do I loop through all pages?',\n",
       " 'Great Intro to this stuff! Nice Video!',\n",
       " 'why are you switching back and forth between CMD and Sublime ? you can view the output in sublime with ctrl + b',\n",
       " 'I would give you trillion likes, i was making huge data scraping and one part of code wasnt working in many req. and you de god of scraping make my week.',\n",
       " 'Amazing tutorial just discovered you but take a breath man lol Looking forward to more of your videos',\n",
       " 'Great video but What to do if multiple pages are there',\n",
       " \"Great video!\\nBut when I tried running the script at at 29:01, the Anaconda Prompt said: 'python: can't open file 'WebScraping_NewEgg': [Errno 2] No such file or directory'\",\n",
       " 'Beautiful !!',\n",
       " \"Hi, I'm trying to scrape something that requires account access. How can I do something like login to the site before scraping?\",\n",
       " 'Really cool stuff!',\n",
       " \"Great vid, once I get to 6:52 after I've named the URL, I get an error stating 'urllib.error.HTTPError: HTTP Error 403: Forbidden. '\\n\\nAny way to get around this?\",\n",
       " 'Holy shit. This is awesome you saved me a lot of time by learning me how to scrape.\\nThank you also for having a pleasent voice to listen to for 30+ minutes :)\\n\\nCheers!',\n",
       " 'Superbly Done.',\n",
       " 'Hey, do you know how I can make this work on sites with Cloudflare DDoS Protection? Because when I run it, I just get the HTML code for the redirecting CloudFlare site.',\n",
       " 'Best tutorial ever!',\n",
       " 'WOW, thanks man, wonderful',\n",
       " 'THIS WAS AMAZING',\n",
       " 'Really good explanation on web scraping. Greate content',\n",
       " \"Ok so I'm following along but when I get to the calling the containers[0] to get the html when i run it in both my terminal or print it in my editor the html just comes out as one long line. Its not formatted at all\",\n",
       " \"Awsome, Good, Excellent, Nice, Best.\\nHope Youtube's algorithm recommend this to every Scrapper Enthusiast.\",\n",
       " 'Great video ',\n",
       " 'I´ve been watching your video and trying to replicate it but the problem is that the web page has a different estructure now. There is a <div> inside that first <a> so when i type container.div it does not go to the <div> below that <a> but rather shows the <div> inside that first <a> how can I tell python to show me the <div> outside that first <a> ? Thanks!',\n",
       " 'Man ! what a video ! Amazing.',\n",
       " 'This was so helpful',\n",
       " \"Hi, do you mind to share how should the codes loop to scrape some contents based on the main category and sub category? Meaning, instead of just a particular url\\xa0(specific page), I would like to scrape and arrange some data based on the main and sub category when I'm on my general url.\",\n",
       " 'I might have missed something in the video. The variable container is not declared in your python file, how can you use it?',\n",
       " 'Thanks. Just watched. Will try tomorrow!',\n",
       " 'Nicely Done!',\n",
       " 'really great video !',\n",
       " 'best python vid for bs4... seriously thank you.',\n",
       " \"So when i input python in the cmd, I get a message saying that the python interpreter is in a conda environment, but it isn't active. I went to look at the environment info but found nothing. I tried to follow the tutorial without fixing the issue, but I can't even install the pip bs4 thingy so idk what to do\",\n",
       " 'Liked it so much, Is there is a way to scrape a page that has a textbox searching and no direct items?',\n",
       " \"BY FAR the best tutorial I've watched for web scraping.\\n\\nBut could someone just help me out with scraping through multiple pages? I know the guy mentioned something about it in the very end but still\",\n",
       " 'Thanks, your video help me a lot.',\n",
       " 'This is a great video! Even as someone relatively unfamiliar with coding and python I was able to follow along a lot of the big picture, I need to learn more basics before becoming totally fluent but this was great. Tyvm for producing this content',\n",
       " 'Very good video! Really quick and straightforward tutorial!',\n",
       " 'Really enjoyed this. One of the most clear, concise and entertaining web scraping videos. Would really like to see more web scraping videos and how to incorporate them into a program that will systematically retrieve and update the information and present in a user friendly format :)!',\n",
       " 'Fast paced and perfect help for those trying this out!',\n",
       " \"LOVED IT MATE! I'VE BEEN TRYING WEB SCRAPING FOR A LONG TIME, GOT THE SYNTAXES AND EVERYTHING FROM OTHER TUTORIALS, BUT THE WAY YOU SHOWED HOW ITS DONE, LIKE THE HANDS ON PROJECT BASED REAL LIFE IMPLEMENTATION AND THE WAY YOU MADE IT WORK CLEARED SEVERAL PROBLEMS I HAD IN MY CONCEPT! FEELING REALLY HYPED THAT I CLICKED ON YOUR VIDEO FROM MY SUGGESTIONS AT 3 45 AM! SUBSCRIBED! PLEASE MAKE MORE DATA SCIENCE VIDEOS! <3 Thank you :p\",\n",
       " 'This tutorial was awesome! Thank you very much! This allowed me to write my first web scrapper without much fuss.',\n",
       " 'More coding videos please! Keep up the great work.',\n",
       " 'Superb!\\nVery useful. Short yet detailed.\\n\\nWould love to see more of such videos.',\n",
       " 'Great video!! I learned a lot. \\n\\nOne question; is it possible to loop through drop down menus? I think that would make a great addition to this video. \\n\\nThanks again!',\n",
       " \"Great! This has been really straight forward. :) I think I won't go on with my crappy web scraping book. And the source code editor is amazing! Thanks a lot!\",\n",
       " 'I really enjoy your teaching style, keep it up and more on scraping, python, coding. Thank you!',\n",
       " 'AMAZING!!!!! you just helped me so much with my school project! please make more videos like that about popular python packages! love this programming intros and tutorials!!',\n",
       " 'Great introduction for web scraping in python using beautifulsoup. Do you also happen to have a tutorial on using for example csv.dictreader to write your csv?',\n",
       " 'Hi I am so excited to see your tutorial! THANK YOU SO MUCH! I have a question how can I write into a CSV file for same information from different links whose webpages have exactly the same structure?',\n",
       " 'Watched this halfway but i was so impressed had to write a comment before finishing it. Thanks for the video! Unlike others where they just code away and make passing statements without really explaining, you did well by at least explaining what you were doing for those familiar with programming but not necessarily with python and its syntax. :)',\n",
       " 'Hell yes I want to see more of this. This was awesome man thanks for putting this up ! All I have to figure out now is how to download the images associated with a real estate listing and my project is Done!',\n",
       " \"Thank you so much, that was an easy to understand Tutorial. I'll definitely rewatch it as I am progressing in my Topic Modeling-Project. I hope this kind of Script'll work for my purpose as well. What I didn't understand was, if I had to download bs4 or is it already implemented in Python/Anaconda? And is there a way to write every output in a single file?\",\n",
       " 'Wow!! Thank you a lot. Been trying to learning web scarping and this video took me a long way. Thank you.',\n",
       " 'very good video. 22:36 you get the a tag\\'s \"item-brand\" because the container itself is the div  \"item-containter\" so instead off findAll you can do container.div.a.text and get the same result faster (explicit tagging is faster than findAll).',\n",
       " 'Thank you, this video was really informative and explained in a clear way',\n",
       " 'Wow! Just completed the tutorial hands on. Already falling in love with Python. Nice work!',\n",
       " 'The best web scraping tutorial I have watched. Would it be possible that you can also demonstrate how to web scrap on the entire site products?',\n",
       " 'man...!!! this was and is the best programming video I have ever watched. I watched tons of videos. Your speed to me was reasonable, voice clear, skill level super and bug free! thanks millions man :) liked, subscribed !',\n",
       " 'Thank you so much. This was an excellent introduction at a level that I could easily understand.',\n",
       " 'Loved the video! This is my first python script and crawler. Thanks a lot brah! Keep them coming.',\n",
       " 'That was great. This is the first coding lesson that I saw useful. Please post more of different projects. Gave you a BIG LIKE!',\n",
       " 'Very helpful Tutorial. Great Job. Just one quick question: What if I wanted to scrape only the first 6 products and not all of them? How would I do that?',\n",
       " \"I love this tutorial! \\nJust what I've been googling for months.\",\n",
       " 'Big thanks for the tutorial! You made it easy to understand.',\n",
       " 'Thank you, this is very helpful and easy to follow.',\n",
       " 'Very Informative and interesting.\\nLooking for more tutorials specially scraping data from html tables.\\nthanks :)',\n",
       " 'Thank you soooo much...... one of the best tutorial I have seen on YouTube... this helps me a lot ️',\n",
       " 'Thank you very much for the knowledge class, I found it very didactic and now I can progress in my projects.',\n",
       " 'Thank you very much! You taught this exactly the way I learn the best.',\n",
       " \"Dude! You're AWESOME!!!! Fast, easy to understand, clear ...and right in point!!!! +1\",\n",
       " 'This is great thank you, really clear and easy to follow. Just one question, how do I scrape a site which requires a password to access it. At work we use a site which requires a password to see our timetable each week, I would like to be able to pull this data into a csv file without needing to access the site in order use the data elsewhere. Is there a video on this?',\n",
       " 'Yes , Loved it.\\nPlease make more videos on Web scrapping. It was fun learning.',\n",
       " 'Thank you ! One question : \"internet becomes an entire database\", is it sometimes illegal to retrieve data from a website ? How many percentage of websites really allow data sharing ?',\n",
       " 'Beautiful, really nice. thank you. Do you know how to do it in a continuous way?  let say each 30 minutes, just extracting the new items added  in those time.',\n",
       " 'Thank you so much! This video helped me a lot.\\nThe explanation is pretty good.',\n",
       " \"This was great. At first i thought I'd just watch the first 10 minutes to get an idea since I am just starting with python, but it was so engaging and simple to follow that I was surprised when i realized I got to the end of the video and wanted more. Thank you!\",\n",
       " 'Great tutorial!!!!\\n\\nAre you going to record the part two about fetching more pages o categories? :)',\n",
       " 'so useful! please, more of this exact same web scraping thing>>>',\n",
       " 'Very nice video! Simple to learn!!! Thanks!',\n",
       " \"Thank you! Spent several hours with Derek Bana's Python primer then your video was the second one. I can't believe it worked first try on almost every step. Only had to reboot after installing Anaconda before getting the bs4 import to not fail. Just the OS needing to update the path.\",\n",
       " 'Incredibly helpful and easy to follow. Now I just have to find a way to run that thing on a server.',\n",
       " 'Thank you for your tutorial. It helps a lot!',\n",
       " 'Fantastic Video! Very practical!',\n",
       " 'Nice video dude\\nJust a suggestion,  to really get introduced with web scraping\\xa0use the entire thing not just importing it as X.\\xa0Just a suggestion...\\nbut yeah great video really opened my eyes to webscraping.',\n",
       " 'Great video, but you should start using jupyter notebook for interactive prototyping. Much easier than copying and pasting from command line.',\n",
       " 'This was very well done. Many thanks',\n",
       " 'exactly what i needed, Thank you....... one of the best explanation tutorial ever...',\n",
       " 'Thanks a lot for the great video.. best video on web scraping. this is what exactly I was looking for :)',\n",
       " \"DUDE, this is the best video I`ve seen, Very detailed, not boring, even python noobie can follow up, I can't believe I watched 33 min without being bored,\\ngj looking for more amazing vids\",\n",
       " \"Thanks a ton! You've made this really, really easy!\",\n",
       " 'Its a very great video for beginners to learn web scraping.',\n",
       " 'I really appreciate this kind of video. Thanks a lot :)',\n",
       " 'Fantastic tutorial.. thats so clear!!',\n",
       " 'Great video, the first one that I was able to follow until the end. \\nI have an issue with my output file as it only prints the headers and the first line. How can I create a container loop so it goes through all available containers. When we defined container = containers[0] I dont see where in the python code that line was added. Thanks for the help and keep it up.',\n",
       " 'I\\'m very new to python. I did this 100%, great tutorial. I am trying to do this on another page now but I keep getting something like \"urllib.error.HTTPError: HTTP Error 403: Forbidden\" after entering the line \"uClient = uReq(my_url)\". From what I\\'ve read it detects me as a bot. So my question is how do I go about  changing the user agent for urllib using this?',\n",
       " 'Great video thanks you. But what do you do next? how do you apply this to your web? or plugin',\n",
       " 'Really nice and clear tutorial. Thanks :)',\n",
       " 'GREAT introduction to webscrapping, thank you',\n",
       " 'Really nice video I learned a lot.',\n",
       " 'THIS WAS SO AWESOME AND SO MUCH FUN! THANK YOU!!!!',\n",
       " \"Geez, I can't believe that I learned so lot in this simple video.\",\n",
       " 'I asked one question on a forum on how to extract data from a website.  One person replied saying web scrape with Beautiful Soup.  Google search found me this video.  I just did my first scrape thanks to this video and man, I just opened myself up to a world of data gathering now!  Thank you for this great beginner video.',\n",
       " \"It's a great video, thank you for this.\",\n",
       " 'Thanks man! I tried to use the same framework on another site and I can\\'t get it to loop through all containers. What am I missing?\\n\\ncontainers = page_soup.findAll(\"div\",{\"class\":\"thumb-block\"})\\n\\nfor container in containers:\\n image_url = container.div.div.a.img[\"data-src\"]\\n \\n title_container = container.p.a[\"title\"]\\n title = title_container\\n \\n vid_num_container = container[\"id\"]\\n vid_num = vid_num_container\\n\\nprint(\"image_url:  \" + image_url)\\nprint(\"title:  \" + title)\\nprint(\"vid_num:  \" + vid_num)',\n",
       " 'what do you do in the case of websites with dynamically generated content? what changes?',\n",
       " 'THANK YOU SO MUCH! It was so good!',\n",
       " 'hey, one question though, what if you need a piece of information that is available only if you open one specific video card page? For example you need the comments, but you can see them only if you open each video card page? Can you go that deep with it?',\n",
       " 'Great tutorial!',\n",
       " 'Awesome tutorial buddy!',\n",
       " 'That is something great !! I loved it.. I learned something :)',\n",
       " 'Awesome video dude. Really, really nice!',\n",
       " 'Enjoyed it! What about if it DOES have an API?',\n",
       " 'Will keep this knowledge handy, if not for e-commerce sites. Thanks.',\n",
       " \"AMAZING TUTORIAL --- QUICK Q\\n\\nDid anyone else have issues with the loop? I've been trying to figure it out but it seems to continue only printing (brand/product) for container[0]. \\n\\nIf I define each container (i.e. 1 or 2 or 3) before running the loop - it changes the outcome accordingly, but still only prints for the one container.\\n\\nAny advice?\",\n",
       " 'Thanks a million for this video !',\n",
       " \"I love excel, now it's time to add python into it :) as a programmer I am used to semicolon :( sad I hated it at one point XD\",\n",
       " 'THANK YOU SO MUCH FOR THE GREAT VIDEO!!',\n",
       " 'I like it so much, thank you :)',\n",
       " 'Well done, Keep on going :D',\n",
       " 'I\\'m surprised the final excel doc didn\\'t have a column for prices..? I only skimmed the video, but I should be able to add a \"price\" column for my own purposes. If you went over this in the video, let me know!',\n",
       " 'it was fun, really enjoyed it!',\n",
       " 'what if I need to get the second div?\\nbecause the html consist of multiple \"div\" in the \"form\", each with different \"id\" or \"class\"',\n",
       " 'How to copy certain contents(it might be image) with formats of HTML to docx?',\n",
       " 'How to copy certain contents(it might be image) with formats of HTML to docx?',\n",
       " 'BEST Tutorial I have ever seen <3',\n",
       " 'My script worked fine... but my output to the csv put everyting in column A, it did not space it into columns A, B and C... PS GREAT tutoria!',\n",
       " 'Sir, how to read this div tag\\n<div class=_3215  row>\\nI am unable to read this using\\nsoup.findAll(\"div\",{\"class\":\"_3215 row\"})',\n",
       " \"hi, i have a question, you wrote container = containers[0] on python console but not in the code script, and it works like magic. How was that happen? is python doesn't need to know that variable information? sorry i'm a noob\",\n",
       " '\"C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\urllib\\\\request.py\", line 649, in http_error_default\\r\\n raise HTTPError(req.full_url, code, msg, hdrs, fp). \\nurllib.error.HTTPError: HTTP Error 403: Forbiddenurllib.error.HTTPError: HTTP Error 403: Forbidden\\nwhat the hell does this mean?',\n",
       " 'You are a badass sir! Thank you for the tutorial.',\n",
       " 'Absolutely Brilliant',\n",
       " 'Hi,\\nhow do I get product url and image url?',\n",
       " 'Is there a better way to find a specific item than Find / Find All? Searching text this way isn\\'t efficient, especially if it\\'s already been processed and extracted into an array or a list. Is there a direct way to get at each element, e.g. by the [\"TagType\"] or another indexed way? Also, you should show how to extract the price :-) Finally, there\\'s no word \"deliminated\". Thanks!',\n",
       " 'Could someone help me - I get 0 when finding len(members) for my page. The relevant line on the inspected page is\\xa0\\n<tr role=\"row\" class=\"odd\"....</tr>\\xa0\\nor\\n<div class=\"member row\" data-member-id=\"1\">',\n",
       " 'AMAZING!!!!',\n",
       " 'This is so awesome! I feel powerful on the internet :P',\n",
       " 'Wow, So cool. I think this is what i need',\n",
       " 'THANKS A LOT. How does this compare to using  miyakogi/pyppeteer\\xa0? Can we use BeautifulSoup for Amazon? I have read we must use pyppeteer\\xa0for that. Thanks',\n",
       " \"Could you do an updated version? They changed the website. And I haven't been able to grab the brand. So, I deleted that part and updated the URL. And it still works.\",\n",
       " 'Basic question: Is that legal? Can you do that without owners permission?',\n",
       " \"that is the best tutorial iv'e ever seen, keep it up, you are the best !!!!!!\",\n",
       " 'sir pls make video on how to update,insert,delete data from sqlite through  fetching it from treeview it would be very  helpfull for many of us pls sir',\n",
       " 'Amazing !!!',\n",
       " \"wow thanks man! that's SO awesome!!!!!\",\n",
       " 'I love the video, make more',\n",
       " 'So I\\'m using \"containers = page_soup.find_all(\"div\", class_=\\'modals-container\\') and no matter which synthaxis I use, it\\'s always giving me zero. Any solution?',\n",
       " 'Nice Tutorial thank you',\n",
       " 'hopefully you read this soon, when i try to do page_soup.h1, or anything, it says:\\n Traceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\nAttributeError: \\'tuple\\' object has no attribute \\'h1\\'  \\nif you know how to fix, please lemme know',\n",
       " 'Can it apply to https page?',\n",
       " 'This was AWESOME',\n",
       " 'At 19:42 when he did \\'container.div\\', all it showed for me was this: \\n\\n<div class=\"item-badges\">\\r\\n</div>',\n",
       " 'Hey how do I get my text editor to show up next to version of python when I type python?',\n",
       " 'Such a great video!',\n",
       " 'Good tutorial but please use pycharm next time',\n",
       " 'This is so much fun!',\n",
       " 'Great video and thanks for walking through the process of figuring out how to extract the specific values, most tutorials and video just skip over that important step.\\n\\nWhat are your thoughts about using XPath vs. beautifilsoup? \\nIt seems to me that XPath would be easier because you can copy the XPath to the field directly from Chrom (right-click).',\n",
       " 'Great teaching method! First I\\'ve seen to actually demonstrate \"visually\" parsing html code and python coding the result. Your testing method is also very helpful. Thanks!',\n",
       " 'That was one of the best web scraping tutorials I have ever seen.  Went step by step through the process of how to determine tags and classes.',\n",
       " \"Thank you so much Phuc!  This was incredibly helpful.  I've never used html, sublime or Beautifulsoup before but you made it really easy to follow.\",\n",
       " 'This was amazing.  I went from not knowing a single line of python to extracting 5000 data objects from a local website today thanks to this video.',\n",
       " \"Wow!  I'm a total programming novice and this tutorial totally made sense.  Great job and thank you for posting!\",\n",
       " 'Thanks for the video!! I have no prior coding background but I\\'m doing a similar project as coursework. The video saves me!\\nI wonder instead of scraping info directly from the graphic cards, can I actually include \"urllib2.urlopen\" in the loop so I can open that graphic card and scrap details for individual web page?',\n",
       " 'Best tutorial on the web.\\xa0 Perfect pacing and great teaching.\\xa0 THANKS!!',\n",
       " 'Hey Dojo, great work and very nice explaining pace. \\nYour script runs smoothly, but i get an error when trying to write my results.\\n(TypeError: coercing to Unicode: need string or buffer, ResultSet found).\\nNo solution found in stackOverflow unfortunately. Any idea how to resolve this issue?',\n",
       " 'Thanks so much for this tutorial, you really have an awesome and understandable way of teaching!!',\n",
       " 'Great Video really helped me, it would be good if u could do more videos on web scrapping and using the data for analytics with python',\n",
       " 'liked, subscribed, added to playlist. \\n\\nThank you Data Science Dojo.\\n\\nAfter months of searching, i found the one reliable source i can count on.\\nNot only you did not waste time, but you were able to explain in a way , someone without a CS degree could undertand..\\n\\nThanks alot',\n",
       " \"Best tutorial I've found so far. A further video to scrape several pages would be great!\",\n",
       " 'That was awesome. You are a great teacher and a pro!',\n",
       " 'Awesome video, awesome content, extremely useful.\\n\\nPerhaps you could put your source-code in a github repository or something like that for those of us have trouble following along at your speed?\\n\\nAll the best & thank you.',\n",
       " 'Thanks man! great tutorial, I learnt quite a lot!',\n",
       " 'This for me has been without a doubt the best tutorial for Web scraping. Extremely well presented and clear. Thanks.',\n",
       " 'Nice! This is so comprehensive. Thank you!',\n",
       " 'Nice video ! You explained so clearly which is rare for programmer video. Keep it up',\n",
       " 'Hey man thanks for the video now you make the whole internet as my database! I learned this to grab some word-meaning-detail such as example sentence, synonym, antonym and sh*t from a dictionary website for my GRE word list. Solved my tedious task and make me learn new super power!!!',\n",
       " \"Hi DataScienceDojo, great tutorial! i've been following your tutorial and testing out some URLs; some of them are throwing a 403 - Fastly Internal error... any idea why?\",\n",
       " 'Although I am not completely through watching this video, you are very helpful and fun to watch Thanks!',\n",
       " \"Awesome stuff! The big take away for me was how you wrote it into a CSV-file.\\n\\nI have a prayer for a video:\\nCan you please make a video on how you can run a script automatically every day? For example how you would make the script you made in this video, run every day at a certain time to collect the data? \\nI have tried cron, but i haven't got it to work yet.\",\n",
       " 'Excellent Video! Easy to follow & works well!',\n",
       " 'This video is incredibly informative. Thank you so much.',\n",
       " 'Great video, I hope to see more Python stuff!',\n",
       " 'That was extremely helpful, thank you!',\n",
       " 'Great video, you should do more videos like this',\n",
       " 'Amazing Phuc Duong! Could you do a tutorial on handling data updated by JavaScript and AJAX? I keep coming across different values.',\n",
       " 'Thank you very much, this was an excellent refresher video!!',\n",
       " \"Thank you for this video... How can I insert a loop statement into  this code (for url pages) so that each page of the website is scraped and how do I include the try statement on this code.. so that fields that doesn't have the keys will show?\",\n",
       " 'This was just great man. however , there are sites with dynamic stuff loaded by javascript which is what bs4 wont see but selenium will. I have heard of guys using Driver.Page_source() to push the full content including javascript loaded contents to bs4 and then parse it. seems to be faster that selenium alone. if you can make a video using this method , and instead of guessing the URL , interacting with the page , clicking buttons to go to next 60 items found for example . than parse in bs4 , it will be extremely appreciated.',\n",
       " 'Very helpful, thank you very much!',\n",
       " 'Really awesome video!!!  Hope you do more Data Science and Web Scraping videos.\\n\\nOne question I do have, is there way to automatically run that script daily without me having to call the program?  So take the example you show, I want to check what they have daily and save it in a folder with different date.',\n",
       " 'This was so helpful! Thank you',\n",
       " 'This helped me scrape craigslist jobs, pull out specific jobs and open the webpage to that job...Your work in action! (Used excel and vba to do the latter, but working on a python variant)',\n",
       " 'Superb video. Very helpful.. Please make more like product analysis through customer sentiments.',\n",
       " 'TQ with good tutorial. \\n\\nBut for me, still the best is preg_match_file using PHP because you can grab between code\\n\\npreg_match_file(\"@front code.?*end code.si\",source,output)',\n",
       " \"Very good information, and easy to follow. One question, what I'm looking for is to scrape all pages within a domain. Pages are all formatted the same, so basically wondering if there is a way to create a loop that will scrape all subdomains, rather than specifying exact URLs. Using the example of New Egg, rather than scraping a single page (or set of pages, /2, /3 etc) it would search all pages within the newegg.com domain. Hopefully that makes sense.\",\n",
       " 'Fantastic Tutorial !\\nThank you',\n",
       " 'Fantastic work keep doing it  man',\n",
       " 'Thank you for explaining the syntax as you went along.',\n",
       " \"Amazing tutorial! Which brings me to this question. You see how you pasted all the info into excel?\\xa0Is it possible to\\xa0paste it into another website? Let's say website 1 is where I gather all info(I want to\\xa0find 200 listings)\\xa0to search then Website 2 gives me Photos + more info(of 200 listings) and now I want to copy and paste 200 of those specific text or photos onto website 3. Can I use the same coding in this tutorial to accomplish this or would I need some additional module or something? Thank you for your wisdom.\",\n",
       " 'awesome!! learned a lot',\n",
       " 'hey the tutorial was cool\\nreally learned some stuff\\nkeep doing such videos',\n",
       " 'I understood very little of this but I watched it all the way through; very interesting.',\n",
       " 'Very good video. Bravo!',\n",
       " 'very educative mate - Thanks',\n",
       " \"At the 28 minute mark how did you manage to copy each of those terms. I'm in pycharm and shift didn't work for me. Nice video by the way as well. I'm looking to make a bot for certain products\",\n",
       " 'Very Nice Video!!!!!!!!!!!! Thanks...Request to upload more Python videos..',\n",
       " \"For those who are wondering what you could do to get the price and other things that some containers don't have, you would you the try: and except: commands.\",\n",
       " 'love it !!',\n",
       " \"If i'm pulling from a webpage using pagination do I have to change anything?\",\n",
       " 'hey what do i do if the html code im obtaining is in one line string?',\n",
       " 'can i get code for scraping in multiple page? thanks in advance',\n",
       " \"If I type 'Python' on the command line, it just asks me to download Python from Microsoft Store. Note: I have already downloaded and installed anaconda as instructed.\",\n",
       " 'how you check for duplicate records in your output. like I want to put only unique records in my csv ?',\n",
       " 'Anybody know if you can use wildcards when searching through the HTML tags for particular words? Like say he only wanted to look for 10 series GPUs can you state only grab \"10**\"? Or is there a more elegant solution to that?',\n",
       " 'Sorry in advance, I am not an expert, I am looking for help. I have followed this code to the \"t\". \\n\\nInstead of graphics cards I am pulling from AMD Ryzen processors on Newegg of which there are 20 in the search. When I input containers and findAll then check the length I do get back \"20\". \\n\\nHowever, I am stuck at trying to loop through and output all 20 items. Each time I go to print (brand, name, shipping) I only receive the output for the first item. I have watched the video over numerous times and have matched the \\'for container in containers:\" loop and outputs exactly. I have tried removing the \\'[0]\\' from product name which then produces an error and instructs me to use find instead of findAll, I have done this as well and it still only outputs the first item. \\n\\nI have looked through a ton of these comments and not found and answer and I have looked at StackOverflow and not found an answer either.',\n",
       " 'how do you know if its an html.parser file? Where can i find that information if i was to be on another site of a different file type?',\n",
       " 'SIr, I followed everything you said but everytime I use from urllib.request import urlopen as uReq  - I get a traceback stating \"*No module named request*\". Please help if anyone know how to solve this.',\n",
       " 'why not just use a web crawler? can do all this with some very minor configuration and one button click',\n",
       " 'So, if I type in containers[0], does it give me every single codes for this class (item-container)?',\n",
       " 'Brilliant - thanks',\n",
       " 'i love this so much, i was just checking out what webscraping is but HOLY MOLY THIS IS SOME BROKEN ASS ABILITY',\n",
       " 'How do I import BeautifulSoup into Python vers 3.8?',\n",
       " 'When I open the command line and type python, I get, \"\\'python\\' is not recognized as an internal or external command,\\noperable program or batch file.\"',\n",
       " \"I've successfully been able to set up my web scrape but my only problem is that every time I run it, it dumps the data into my excel sheet a million times. As the code runs on my command prompt it shows that the data is collected only once, so I don't see it it's collected numerous times on the excel sheet... Please help!\",\n",
       " 'Awesome video!',\n",
       " 'This was one hell of video , thanks',\n",
       " 'I get an error when I am typing len(containers) it is showing 0 instead of 12 .',\n",
       " 'Excellent, God Bless You.',\n",
       " 'EXCELLENT VIDEO!!',\n",
       " \"I'm doing a web scraping project for our company. How can I make an executable file out of this?\",\n",
       " 'python gives syntax error for \"for containers in containers:\". Anyone have a suggestion as to why this is happening? Thanks in advance.',\n",
       " 'very helpful try to do more this gon help everyone.',\n",
       " 'amazing!!!',\n",
       " 'when i print containers[0] only a small part of the html is shown. All the contents of the class item-info and item-branding is hidden. How can i solve this?',\n",
       " \"Some people don't get it, but being able of doing this is fucking awesome.\",\n",
       " 'amazing video thank you',\n",
       " 'Man, you are awesome! :)',\n",
       " \"Use conda instead of pip when using anaconda! (It's better anyway)\",\n",
       " 'I like the way of your teaching.',\n",
       " 'This is great',\n",
       " 'How to scrape data on multiple page (next page)?',\n",
       " 'Having problems in this line\\nbrand = container.div.div.a.img[\"title\"] it comes back with the following error\\nTraceback (most recent call last):\\n  File \"/home/john/Documents/DATA ANALYSIS/video_webscrap.py\", line 33, in <module>\\n    brand = container.div.div.a.img[\"title\"]\\nAttributeError: \\'NoneType\\' object has no attribute \\'a\\'\\n\\n\\nThank you for your input!',\n",
       " '23:02 that sound when you do the coma :D Very good tutorial! Thanks a lot!',\n",
       " 'Hi would you recommend using urllib or requests?',\n",
       " 'Great video fam!',\n",
       " 'Good video, thank you',\n",
       " \"You're amazing!!!\",\n",
       " 'The solver mind is amazing ',\n",
       " 'This is the closest I will get to hacking ...lol\\nNice tut , thanks',\n",
       " 'When I try to do a variable.div.a it tells me \"ResultSet object has no attribute \\'div\\'\"',\n",
       " 'You know that there are beautify/prettify html/css/js plugins for Sublime, right? Why would you paste it into an online beautifier when you can just use a hotkey in Sublime??',\n",
       " 'Hi, could anyone give me a hand on this? this line: brand = container.div.div.a.img[\\'title\\'], throws this error: \\nTraceback (most recent call last):\\r\\n  File \"C:/Users/nelso/PycharmProjects/Scrapping/Scrap001.py\", line 23, in <module>\\r\\n    brand = container.div.div.a.img[\\'title\\']\\r\\nAttributeError: \\'NoneType\\' object has no attribute \\'a\\'',\n",
       " 'Nice Video. great! Helped me a lot!!!',\n",
       " \"how do you avoid being blocked by those websites since you're scraping quite often?\",\n",
       " \"I'm struggling to create brand. I get an error working on the for loop. Please try to put code at the centre of screen rather than where the subtitles are... or remove subtitles\",\n",
       " 'I know, 1.3 million viewers do not lie, but for me the video would work better if the code would be visible throughout the video - as half screen view.',\n",
       " \"I recommend checking Corey Schafer's web scraping tutorial first and then come back to this project, you'll finish it quicker.\",\n",
       " 'the best video. thanks!',\n",
       " 'A ton of gratitude from here!!',\n",
       " 'I need to scrape data from different websites so how I can I do that in this script.\\nAlso either any method to pass a text document to the \"my_url =  \" So I can put all the websites in that text document and then pass it to the my_url variable to scrap from those sites and store in a csv file.',\n",
       " \"That 'https' in your browser prevents your internet service provider from collecting a lot of valuable data, although they can still collect the addresses of your visits. I predict a global browser add-on that collects anonymous website data such as product prices and reward the users in some fashion. And think hard about the add-ons the already you have installed, by virtue of their permissions have the capability. For example, those recent ad-blocker ads, that would be a great scrape tool unless they run you out of data on your phone.\",\n",
       " 'best teacher ever',\n",
       " 'Can we use xpaths in soup?',\n",
       " 'I love your energy',\n",
       " \"so what to do when you've got to scrape the whole website? i mean like all the pages? do u have to go to each page individually and scrape them?\",\n",
       " 'I loved this so muchhhhhh !!!! ^^',\n",
       " 'Nice Tut as it goes, kudos.',\n",
       " 'the first element of your container did not match with the first element of my container. It is also counting in the adds that are shown! what should i do?',\n",
       " 'I have no idea what happened but I enjoyed it...maybe I should learn Python.',\n",
       " 'when I run page_soup.h1, I get no error, but I also get no answer (this is for a different page than the one in the video).  What do I do now?',\n",
       " 'This just saved my life <3',\n",
       " 'Brooo this is so sick thank you!!!',\n",
       " 'How do I update the information that is inside of excel file automaticaly?',\n",
       " 'AMAZING',\n",
       " 'Great tutorial ..',\n",
       " 'Bruh. Why do you only have less than 50K subs. You deserve millions. Thx dude!',\n",
       " 'You should publish on pluralsight',\n",
       " \"Does anyone know why I only get the first index when i run the my_first_webscrape.py file? It doesn't loop through all the products. It only gives me one at a time\",\n",
       " 'What a legend!',\n",
       " 'This is epic',\n",
       " 'I learned a lot in this video, however, your erratic back and forth, short-handing stuff, and glossing over things quickly made it very hard to follow. I can\\'t tell you how many times I had to pause the video and back up a few seconds to see what you did. \\n\\nFor example, in the first section on sublime text, you state you type in \"set syntax python\" and before you finish typing it out, you backspace and start writing something different and hit enter before you finish typing it. You were just short-handing the statement to \"Syntax Pyt\" but I had to back up and read what it was saying because your actions were different than what you were actually saying you were doing. \\n\\nWhen you were copying and pasting things and quickly going back and forth from the console to sublime text and it was confusing to follow you that quickly and make sure I had everything written correctly.\\n\\nOtherwise, keep up the good work. Thanks!',\n",
       " 'can i use anaconda prompt instead of the command prompt',\n",
       " 'this is badass, seems easier than\\n cheerio and request',\n",
       " 'Great Job !!',\n",
       " 'We loved it babe,thanks!',\n",
       " \"hey, bro I still don't know how to scrape the rates, because as you say there is some products that don't have any rate so we need to add some if conditions but I don't know how to do it.\\nplease reply.\",\n",
       " \"Drinking Game : Take a shot everytime he says M'kay. \\n\\nPS: Thanks for the great video <3\",\n",
       " 'When I try to do the control c as shoe at the very beginning of the video I get a “keyinterrupt” message, any help on this?',\n",
       " \"I really dont know what's happeningbut still want to learn this kind of stuff.\",\n",
       " 'the black and green cmd with a massive font makes stuff harder to read and follow. \\nyou probably want to make it easier to follow within a video, but it does the opposite',\n",
       " 'Awesome video. Just subscribed.',\n",
       " 'This was so fun!!!! Dzamn',\n",
       " 'Great Video man!!',\n",
       " '31:40 - \"Deliminindated\" RIP\\n\\nThanks for the vid mango',\n",
       " 'Killing me. I can’t tell you how many vid’s I’ve watched in the last 24 hours on Raspberry Pi, Linux, etc... where the narrator / tuber, slowly covered things like “This is the box”, “This is me opening the box” while I was screaming at the monitor, “Seriously dude, seriously?”... Then I choose a somewhat more complex topic, significantly, and I get “Larry The Coffee Head” speaking as fast as my daughter when she flips out, scrolling like a madman, flipping back and forth between command line and shell and web page, with hardly a breath in between the elements he’s describing. Dude, so far? Great stuff but MY GOD PLEASE SLOW THE HELL DOWN!  Thanks for the effort, I’m way ahead of where I was before I pressed play, and I’m glad you aren’t monotone going through insane details like, type this line, then hit enter. But there is a common ground in between. Please seek it. Thank you.',\n",
       " 'Subscribed, Thank you very much you are awesome',\n",
       " 'dude was being chased by a cheetah, great content btw thanks a lot',\n",
       " 'What about bypassing Captchas on websites?',\n",
       " 'So, I see Superman is alive and well. GOOD JOB!',\n",
       " 'Everything is great just the voice, bearing it',\n",
       " 'Really awesome tutorial 1+ suscriber',\n",
       " 'Does python understand plural or something? He never sets a variable for container.',\n",
       " 'great video\\nwe want more',\n",
       " 'File \"newwebscrapper.py\", line 36, in <module>\\r\\n    brand = make_rating_sp[0].img[\"title\"].title()\\r\\nTypeError: \\'NoneType\\' object is not subscriptable',\n",
       " 'hi there - great stuff i am overwhelmed - one question - what if i run this code today - is this doable?',\n",
       " 'Simple and Clear',\n",
       " 'I Enjoyed watching it bro. do more',\n",
       " 'Thank you so much !',\n",
       " 'Thank you for a good video',\n",
       " 'Why do i keep getting \"  File \"<stdin>\", line 1, in <module>\\nTypeError: \\'NoneType\\' object is not callable\" \\nfor  page_soup.findALL(\"div\",{\"class\":\"item-container\"})?',\n",
       " 'Any one can tell,  how to find number of occurrence of a particular word in a given website using python beautiful soup...',\n",
       " 'Awesome!!',\n",
       " '9:00 Could have just called \"page_html[:100]\"',\n",
       " 'I\\'m having trouble. I\\'ve installed anaconda, but when I access the command prompt and I type \\'python\\' it returns an error. I also cannot find the \"webscrape\" folder you are using, and if we have to make it, why did you place it in downloads? Also, when I shift-right-click it asks me to open the powershell which also returns an error.',\n",
       " \"AttributeError: 'NoneType' object has no attribute 'a'\",\n",
       " 'how to select a date in a datepicker on a webpage using python?',\n",
       " 'Nice tutorial',\n",
       " 'Hi tanks for perfect tut about bs4 , i just a question about how i can login to web site and crawl the page some web sites need to login to access data . can you tell me how can i do it???',\n",
       " 'when I type container.div                       it  actually print <div \"class=badges\"> in the terminal.  Could you teach me how to fix it?',\n",
       " 'Excellent video',\n",
       " 'Thank you now I can start with with scraping',\n",
       " 'BeautifulSoup Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\\nis anyone facing this problem ?',\n",
       " 'Thanks , liked it.',\n",
       " 'Amazing Video!',\n",
       " 'YOU ARE THE BEST, THANK YOU SO MUCH',\n",
       " \"I have a webscrapping use case which I need help with - \\nSo, I have a list of blog posts that  I have written on a particular website, I was to scrape them all and put it on my portfolio website (my personal webpage), I want to automate this I mean embed the webscrapping script in my portfolio website's html so that everytime I write a new blog on that site my portfolio webpage gets appended for the list of blogs I have written. How do I go about? I don't think the CSV way works\",\n",
       " 'Radical! thank you',\n",
       " 'Fantastic !',\n",
       " 'Awesome!',\n",
       " 'i follow step by step but it not works with me i have this error \\n File \"webscrap.py\", line 22\\n    product_name = title_container[0].text\\n                                         ^\\nIndentationError: unindent does not match any outer indentation level',\n",
       " 'thank you my guy, you rock!',\n",
       " 'brand = container.div.div.a.img[\"title\"] does not work anymore.\\nI tried alt as well and it doesn\\'t work. Anyone know how to fix this?',\n",
       " 'loved this tutorial and it was super easy to understand. Couldnt help but laugh when i saw rape.py at 31:57 haha',\n",
       " 'Can I use this to scrape business info from Yelp for all California restaurants? Or do you recommend using API?',\n",
       " \"Ok, I followed through and reached that magical moment when I'd put in the name of my script in the console, AND:\\n\\nError messages all over the place....\",\n",
       " 'I have a question. How to web Scraping with a lot of pagination? Please give me a solutions. Thanks.',\n",
       " 'Good one , thanks',\n",
       " \"Hi, For me the issue is, i can't write headers to excel file..\",\n",
       " 'hello, found this video very helpful to the most part. Im a complete bot with python and im just learning it. the uClient keeps coming up wit herrors can anyone help?',\n",
       " 'wow,  i love this, wow thank you',\n",
       " 'i take its an example ,when iam doing it showing len(containers)is zero . can u pls  give me the solution',\n",
       " 'Urgent question! is it possible to click on a \"show text\" button before scrapping data from a given url? how do i do that?! please help =(',\n",
       " \"What if it's on a different page? Do I have to run it again\",\n",
       " 'what does container[0] do here exactly, why we take index as 0 not any other number?',\n",
       " 'I\\'m bummed, this looks so simple but I keep getting an error on this part:\\n\\nfrom urllib3.request import urlopen as uReq\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\nImportError: cannot import name urlopen\\n\\nAny idea how to fix?',\n",
       " 'very useful video....thanku',\n",
       " 'Happily subscribed, thanks for sharing!',\n",
       " 'I am trying to data scrap on yahoo finance. \\nI am facing an error \"Index out of range\" while trying to pull the data from the right end side of the site which gives the company details and description.\\nperiod = soup.find_all(class_=\"D(ib) W(47.727%) Pend(40px)\")[0].get_text() \\nPlease help me to resolve this issue.',\n",
       " 'nice, this is awesome.',\n",
       " 'i\\'m newbie. please help me how to fix \\' line 216, in init \\n    % \",\".join(features))\\r\\nbs4.FeatureNotFound: Couldn\\'t find a tree builder with the features you requested: lzd.parser. Do you need to install  \\' . I tried to install the parser but failed. I am using python version 3.6. so thanks you!',\n",
       " \"I'm trying to web scrap weather station, but I don't have numbers in my HTML code...? Continuously messing the value of the actual temperature. Any tricks?\",\n",
       " 'HELP:\\nshipping_container = container.findAll(\"li\", {\"class\":\"price-ship\"})\\r\\n    print(shipping_container[0].text)\\n\\nTraceback (most recent call last):\\r\\n  File \"scrapper2.py\", line 19, in <module>\\r\\n    print(shipping_container[0].text)\\r\\nIndexError: list index out of range\\n\\nso there is no object 0?\\nbecause when i do this:\\n\\nshipping_container = container.findAll(\"li\", {\"class\":\"price-ship\"})\\r\\n    print(shipping_container)\\n\\nASUS TUF Gaming GeForce GTX 1660 SUPER Overclocked 6GB Edition HDMI DP DVI Gaming Graphics Card (TUF-GTX1660S-O6G-GAMING) \\r\\n[<li class=\"price-ship\">Free Shipping</li>]\\n\\ni get that result. How can i acess the text inside the li tag ?\\nObviously i want: Free Shipping',\n",
       " 'Is the file products.csv should be created prior to writing the code ?',\n",
       " \"Is it possible to scrap facebook with this Soup thing? Having in mind that facebook doesn't load all the information at once on it's timeline.\",\n",
       " 'It is a good tutorial yes :)',\n",
       " 'awsome web scrapping tutorial',\n",
       " 'good stuff. Thanks',\n",
       " ' thanks a lot',\n",
       " 'Need Help:\\nI am scraping a web-page for all the anchor tag links. I want to find the link at nth position from the first link on that webpage and then access the nth link. How do I do it?',\n",
       " 'after i complete the following code: page_html = uClient.read()\\n\\ni get the following error: ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host.\\n\\nanyone know how to debug this?',\n",
       " \"Ok Now I'm stuck. I'm trying to pull data but a specific data has several sentences in it. Now I'm getting result where the sentence breaks up and is spilling into the next csv cell instead of keeping all the text in one cell. How can I make sure the stuff stays together? I tried .split() and few others but can't get it to work.\",\n",
       " '<option data-formated=\\'&lt;span class=\"price\"&gt;AUD $100.08&lt;/span&gt;\\' data-qtyid=\"qty-219\" value=\"1\">\\r\\n                                        Unit Price                              </option>\\n\\n\\nhow do i get AUD $100.08 out of this?',\n",
       " 'Great video',\n",
       " 'Best tutorial',\n",
       " 'How do I data scrape a site that requires sign in first before you can access infos?',\n",
       " 'Does anyone else get this error: `urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate` while running the `uClient = uReq(my_url)` command?',\n",
       " 'Hi I keep getting this error please help\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-5-76a3a66d2039> in <module>\\n     23 \\n     24 for container in containers:\\n---> 25     brand = container.div.div.a.img[\"title\"]\\n     26     title_container = container.findAll(\"a\", {\"class\": \"item-title\"})\\n     27     product_name = title_container[0].text\\n\\n\\nTypeError: \\'NoneType\\' object is not subscriptable',\n",
       " \"Do you have a video that shows how to program a bot so I can scrape a webpage that's behind a login-esque wall.\",\n",
       " 'I’m using a Mac to scape data from a html page that I’m needing to bring into a Honeywell Tridium Jace. Can you show how to do that?',\n",
       " 'I did container.div.div.a and got \"\\'NoneType\\' object has no attribute \\'a\\'\"',\n",
       " 'Maybe try to explain things a little bit more. By the way, great video!',\n",
       " 'containers=page_soup.findAll(\"div\",{\"class\": \"item-info\"})  \\r\\nprint(len(containers))\\n\\nthese two lines are not giving back the list of item, print statement is giving 0 instead of 12. why is that..???',\n",
       " 'What if i don’t have excel ? Is their another alternative I can use? Thank you',\n",
       " 'i cannot loop the variable  inside the container \\nany ideas?\\nhere\\'s my code\\ncontainers = page_soup.findAll(\"div\",{\"class\": \"item-container\"})\\r\\ncontainer = containers\\r\\n \\n\\rbrand = container.find(\"div\",{\"class\": \"item-branding\"}) <<<<---------------------------------- container is asking for [] , but how can i loop this\\nmodel = brand.a.img[\"title\"]\\r\\n \\nprint(model)',\n",
       " 'Where is the documentation for this?',\n",
       " 'Does it works for Dynamic web?',\n",
       " 'Thanks pal Just what i need can i do the same and save the data on a database',\n",
       " \"So I have question whenever the sites update it's info will the code work and show us new results?\",\n",
       " 'THANK YOU!!',\n",
       " 'I just use lxml, because more faster and stable.',\n",
       " 'Doesnt this work with google.com search results? I wanted to work with google shopping link from where I could fetch images and data.. but its giving me error as \"HTTP Error 403: Forbidden\"',\n",
       " 'Since u installed anaconda, Y dont u use one of the  many IDE´s avaliable there ?? I guess spyder is the most conftable in the set...',\n",
       " \"Does it really have to be anaconda? Can't I use a regular python for data scraping?\",\n",
       " 'Sir I need learn web scraping in php please make some video',\n",
       " 'When I do the length of the containers it says zero. Can someone help',\n",
       " 'when i save the script- its giving me an error. for price shipping its saying - \"inconsistent use of tabs and spaces in indentation\"\\ncan someone pls help',\n",
       " 'Great!',\n",
       " 'Wow thank you very much',\n",
       " \"why do I always get an empty list when doing the findAll function? Even though im trying it on the same Website? I just don't get it\",\n",
       " 'Thank You So much \\nu are awesome <3',\n",
       " 'Really I m so match happy, thanks for you',\n",
       " 'it says python is net recognized as an external or internal command in the command prompt',\n",
       " 'keep going mr Phuc, làm video dạy mình cách lấy coupons and deal của các site lazada, shopee, tiki đi anh, tks a',\n",
       " 'How to get data with pagination? Not only in a page but in page 1 2 3 4 5.....',\n",
       " \"I keep getting 'return codecs.charmap_encode(input,self.errors,encoding_map)[0]\\nUnicodeEncodeError: 'charmap' codec can't encode character '\\\\xae' in position 104808: character maps to <undefined>' when I try to run the script in the cmd.\\n\\nDo you know what the souce of this problem might be?\",\n",
       " 'great video! I raised the view count by 500 after spending hours scraping videos from youtube LOL',\n",
       " 'How can i use web scraping in Android Studio?',\n",
       " 'sir any chance for instagram daily posts directly importing to our html site with any tool',\n",
       " 'When i type in python it just says python is not regonized as an internal or external command. Same with the 2+2',\n",
       " \"Can someone post their final script code please? I got lost because the code doesn't work anymore in this video.\",\n",
       " \"On 26:29,  why those return and new line shows up, but when I tried it myself it didn't?  There's no \\\\r and \\\\n in the html but it shows up in his command line. I'm a beginner so I might missed something\",\n",
       " 'could you upload the script?\\nWhat about this scrip yellowpagesspider.com',\n",
       " 'UClient = urlopen(LINK)\\n\\nERROR :raise HTTPError(req.full_url, code, msg, hdrs, fp)\\nurllib.error.HTTPError: HTTP Error 403: Forbidden',\n",
       " 'THANK YOU SO MUCH',\n",
       " 'Can You do price of gc - because have only sheeping costs ;)',\n",
       " 'Fantastic Video',\n",
       " \"python: can't open file 'filename.py': [Errno 2] No such file or directory\\nfor some reason, im getting this error while running it\",\n",
       " 'At 26:48, free_shipping[0].text takes the \"TEXT\" out of tag. How do the same in case of numbers??',\n",
       " 'This is so cool',\n",
       " 'didnt work even after copy pasting the code. first the html code wont parse and it onlz returns a big one string line of html and then when i try to write to the csv file it wont give me permission to write to csv. then, i changed to \"with open\" and tells me that it cannot be done as im trzing to add non descriptible objects',\n",
       " 'Please requests library instead of urllib if possible. Its easier I guess?',\n",
       " 'Its good but too complicated !!',\n",
       " 'after i type page_soup.h1 nothing renders.. anyone has advice?',\n",
       " \"doesn't work with amazon though, error msg says HTTP Error 503: Service Unavailable\",\n",
       " 'Great video ..',\n",
       " 'Great Video <3',\n",
       " 'how do you open a command prompt for Mac OSX users?',\n",
       " 'I got an error in coding, IndexError: list index out of range I wonder why? can someone help',\n",
       " 'Im using a website which updates as you scroll down the website. This causes me to only get 20 objects instead of the supposed ~200 that exists. How does one solve this?',\n",
       " 'Ok, yeah, I\\'m a rookie --- getting an error when I type \"pip install bs4\" --- what to do? Thanks',\n",
       " 'Could my IP get banned if I try this in other websites? Great tutorial',\n",
       " 'good stuff sir',\n",
       " 'Your solution for brand doesn\\'t work for me... Maybe this site has changes a bit... here\\'s my solution for it\\n\\n\\nbrand_container = container.findAll(\"img\", {\"class\": \"lazy-img\"})\\nbrand = brand_container[1].get(\\'title\\')',\n",
       " 'Top Video\\nThanks for sharing',\n",
       " \"So unfortunately I'm getting the message HTTP Error 403: Forbidden\\nAccording to RFC 7231: The 403 (Forbidden) status code indicates that the server understood the request but refuses to authorize it. Is there a way to get around this?\",\n",
       " 'I don\\'t think this code is correct. To my understanding, you\\'ve written \"container\" instead of \"containers\" in lines 18, 21, 23. Please let me know if I\\'m wrong.',\n",
       " 'Data Science Dojo Çok Hoş Duruyor, Hemen Kendim Denemeliyim...',\n",
       " 'perfect',\n",
       " 'Oh My...how long does it take to get to this level?',\n",
       " 'brilliant.... no wonder u guys lead in IT',\n",
       " 'Awesome',\n",
       " 'After scraping how I can implement in flask',\n",
       " 'I get an syntax error when I put the \"pip install bs4\" in the cmd',\n",
       " 'I am using python3 and I am getting error urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)>',\n",
       " 'thank you sir',\n",
       " 'big thanks i can recomment this vid 10/10! youre a g',\n",
       " 'I already installed anaconda but when I type \\'python\\' why does it says \"the term python is not recognized as the name of the cmdlet...\"?',\n",
       " 'Why does my command promt doesn\\'t recognize python and I looked further and I discovered cmd.exe promt inside anaconda where it runs python perfectly but when I do pip install bs4, it doesn\\'t again recognize, it says File \"<stdin>\", line 1 pip install bs4 error invalid syntax. Please help :( I can\\'t even start this whole tutorial',\n",
       " 'I ran the python code and got an error: IndexError: list index out of range.\\n\\nIt occurred when at:  brand = make_rating_sp[0].img[\"title\"].title(). \\n\\nHow do I fix this?',\n",
       " 'How can I scrap all reviews brandwise from Amazon page?',\n",
       " 'I went through this problem. Does anybody have any idea, thanks in advance!\\n\\nTraceback (most recent call last):\\r\\n  File \"C:\\\\Users\\\\NQG\\\\Downloads\\\\webscrape\\\\webscraping.py\", line 18, in <module>\\r\\n    brand = container.div.a.img[\"title\"][1:]\\r\\nTypeError: \\'NoneType\\' object is not subscriptable',\n",
       " 'i like your content.',\n",
       " 'please help me.\\nSave as csv but only 1 column !!!\\nHow can I make 3 columns?     [32:44] [minute,second]',\n",
       " 'Does anyone know how to download beautiful soup or python requests',\n",
       " 'You are perfect.',\n",
       " 'Awesome man',\n",
       " 'i would love to do this, it’s super useful to me right now, but is this legal?',\n",
       " 'when i do the print, all the data puts at large on A, why is not printing on excel the data on list?? anyone knows??',\n",
       " 'I am trying to scrape a website with but the \"url\" i copied from the site is not returning any item, what could be the cause. (\\' https://www.konga.com/category/macbooks-5249 \\') That\\'s the web_url.. Thanks',\n",
       " 'glad to know you are VietNamese, tieng anh gioi lam bro :)',\n",
       " 'Is it normal that when I call uReq, for 90% of URLs it return an error?',\n",
       " 'what if i want print only 15 link that i scrapped?',\n",
       " 'awesome intro @dojo',\n",
       " 'I suggest to make video with pandas and API.',\n",
       " 'enjoyed. thanks',\n",
       " 'Unable to get shipping? what should I do?',\n",
       " \"I've encountered problem with the user agent but I was able to solve that by specifying a fake agent. When i reach the page_html step where it sets the html.parser, then I couldn't read anything from any elements like h1. Nothing gets return.Then  I change my url to the one this video shows, the h1 was returned. What is possibly the problem? Could it be the https page that I am trying to request from? Please help :(\",\n",
       " 'URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)> \\n\\nI can not open the URL ',\n",
       " 'i just can say \"thank you sooooooooo much\"',\n",
       " 'why am I getting this error?\\n brand = cop.div.div.a.img[\"title\"]\\r\\nTypeError: \\'NoneType\\' object is not subscriptable',\n",
       " 'unfortunately i found this error when calling the html.parser. the error is \" bs4.FeatureNotFound: Couldn\\'t find a tree builder with the features you requested:  html.parser. Do you need to install a parser library?\\r: how to solve this error????',\n",
       " 'I got a 403 Forbidden error, was wondering if anyone ran into the same thing?',\n",
       " 'I have question , for each command I created , there is an error ( not found) , how I can solve this?',\n",
       " 'Perfect...',\n",
       " 'No need for terminal in sublime ctr+b with run it like the terminal',\n",
       " 'When I open excel it puts all information in one row, how do I solve this?',\n",
       " 'What can i do if i got hit with this error , urllib.error.HTTPError: HTTP Error 403: Forbidden?',\n",
       " 'Very nice….',\n",
       " 'great video but why so zoomed in on text? i legit thought this was at a conference or something for the first half',\n",
       " 'Thanks for this ',\n",
       " 'i really enjoyed it ...thx ubro',\n",
       " 'Nothing different if I use Visual Studio Code to write the script?',\n",
       " 'when I tried to do uclient.read it came out as an incomplete read . what do I do',\n",
       " 'great vid',\n",
       " \"my urlopen('url.example') runs indefinitely and thus never executes uClient.read() - how can I solve this?\",\n",
       " 'Thanks you are TOP',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea50627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afaf1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('YouTube_WebScraping.csv', 'w', encoding='utf-8') as data:\n",
    "    writer = csv.writer(data)\n",
    "    writer.writerow(comment_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
